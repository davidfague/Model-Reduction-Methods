{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidfague/Model_Reduction_Methods/blob/main/New_NMDASimulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9de89f",
      "metadata": {
        "id": "7a9de89f"
      },
      "source": [
        "# Simulation for generating data for dendritic spike analysis\n",
        "##Recorded Currents:\n",
        "\n",
        "Na,K,Ca,ih,...\n",
        "\n",
        "## synapse distribution:\n",
        "\n",
        "10,000 random. will update to include more realistic algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSX7LmGzszzf",
      "metadata": {
        "id": "MSX7LmGzszzf"
      },
      "source": [
        "#### Download modules from Github"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neuron"
      ],
      "metadata": {
        "id": "-urE9jrxNdxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7822a2c6-dddb-424e-e1ad-16c911f3c57a"
      },
      "id": "-urE9jrxNdxm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neuron\n",
            "  Downloading NEURON-8.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.9/dist-packages (from neuron) (1.22.4)\n",
            "Installing collected packages: neuron\n",
            "Successfully installed neuron-8.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neuron_reduce"
      ],
      "metadata": {
        "id": "BHjnRgPsN0g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e747fc1-6f23-49ad-a0a5-3b17dfc2f35b"
      },
      "id": "BHjnRgPsN0g2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neuron_reduce\n",
            "  Downloading neuron_reduce-0.0.7-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: neuron_reduce\n",
            "Successfully installed neuron_reduce-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/davidfague/Model_Reduction_Methods.git"
      ],
      "metadata": {
        "id": "t4dVzA0WLZ9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88515574-c8f4-40b2-e3ae-14063392bff2"
      },
      "id": "t4dVzA0WLZ9F",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Model_Reduction_Methods'...\n",
            "remote: Enumerating objects: 932, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 932 (delta 92), reused 0 (delta 0), pack-reused 774\u001b[K\n",
            "Receiving objects: 100% (932/932), 6.00 MiB | 8.18 MiB/s, done.\n",
            "Resolving deltas: 100% (521/521), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Model_Reduction_Methods/\n",
        "\n",
        "#import reduction and expansion functions\n",
        "from test_neuron_reduce.subtree_reductor_func import subtree_reductor\n",
        "from cable_expander_func import cable_expander\n",
        "\n",
        "#import recording functions\n",
        "from stylized_module.recorder import Recorder\n",
        "\n",
        "#import analysis functions\n",
        "from utils import make_seg_df,generate_stylized_geometry,make_reduced_seg_df,plot_morphology,check_connectivity,generate_reduced_cell_seg_coords, create_seg_var_report,plot_seg_heatmap\n",
        "\n",
        "\n",
        "# from modeling_module.synapses import Synapse, Listed_Synapse\n",
        "from modeling_module.cell_model import cell_model\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cY9U_KPwLVAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75483717-2c35-4c56-f347-ec1c74ed8346"
      },
      "id": "cY9U_KPwLVAz",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Model_Reduction_Methods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neuron import h\n",
        "from typing import Optional, Union, List\n",
        "from modeling_module.synapses import CurrentInjection, Synapse, Listed_Synapse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import h5py\n",
        "import csv\n",
        "class cell_model():\n",
        "  '''expanded cell model class for ECP calculation\n",
        "  takes hoc cell model and does bookkeeping for analysis functions\n",
        "  '''\n",
        "  def __init__(self,model,synapses_list=None,netcons_list=None,gen_3d=True,gen_geom_csv=False,spike_threshold: Optional[float] = None):\n",
        "    self.all=model.all\n",
        "    self.soma=model.soma\n",
        "    self.apic=model.apic\n",
        "    self.dend=model.dend\n",
        "    self.axon=model.axon\n",
        "    #convert nrn section lists to python lists if applicable\n",
        "    self.all=self.__convert_sectionlist(sectionlist=self.all)\n",
        "    self.soma=self.__convert_sectionlist(sectionlist=self.soma, return_singles=True) #if sectionlist contains only one section returns just the section instead of list of sections\n",
        "    self.dend=self.__convert_sectionlist(sectionlist=self.dend)\n",
        "    self.apic=self.__convert_sectionlist(sectionlist=self.apic)\n",
        "    self.axon=self.__convert_sectionlist(sectionlist=self.axon)\n",
        "    self.spike_threshold = spike_threshold\n",
        "    self.synapses_list=synapses_list #list of synapse objects from model reduction\n",
        "    self.netcons_list=netcons_list # list of netcon objects from model reduction\n",
        "    self.segments=[] # list for nrn segment objects\n",
        "    self.injection=[] # list of injection objects\n",
        "    self.synapse=[] # list of python synapse class objects\n",
        "    self.sec_id_lookup = {}  # dictionary from section type id to section index\n",
        "    self.sec_id_in_seg = []  # index of the first segment of each section in the segment list\n",
        "    self.sec_angs = [] # list of angles that were used to branch the cell\n",
        "    self.sec_rots = []\n",
        "    if gen_3d==True:\n",
        "      self.__generate_sec_coords()\n",
        "    self.__store_segments()\n",
        "    self.__set_spike_recorder()\n",
        "    self.__calc_seg_coords()\n",
        "    self.__store_synapses_list() #store and record synapses from the synapses_list used to initialize the cell\n",
        "    self.grp_ids = []\n",
        "    if gen_geom_csv==True:\n",
        "      self.__generate_geometry_file()\n",
        "    # self.calculate_netcons_per_seg()\n",
        "    self.__insert_unused_channels()\n",
        "    self.__setup_recorders()\n",
        "\n",
        "  def __calc_seg_coords(self):\n",
        "      \"\"\"Calculate segment coordinates for ECP calculation\"\"\"\n",
        "      p0 = np.empty((self._nseg, 3))\n",
        "      p1 = np.empty((self._nseg, 3))\n",
        "      p05 = np.empty((self._nseg, 3))\n",
        "      r = np.empty(self._nseg)\n",
        "      for isec, sec in enumerate(self.all):\n",
        "          iseg = self.sec_id_in_seg[isec]\n",
        "          nseg = sec.nseg\n",
        "          seg_length = sec.L / nseg\n",
        "          for i in range(sec.n3d()):\n",
        "              pt1 = None  # initialize pt1\n",
        "              if i == 0:\n",
        "                  pt0 = np.array([sec.x3d(i), sec.y3d(i), sec.z3d(i)])\n",
        "              elif i == sec.n3d() - 1:\n",
        "                  pt1 = np.array([sec.x3d(i), sec.y3d(i), sec.z3d(i)])\n",
        "              else:\n",
        "                  pt0 = np.array([sec.x3d(i - 1), sec.y3d(i - 1), sec.z3d(i - 1)])\n",
        "                  pt1 = np.array([sec.x3d(i), sec.y3d(i), sec.z3d(i)])\n",
        "              if pt1 is not None:  # only set pt1 if it was initialized\n",
        "                  for j in range(nseg):\n",
        "                      p0[iseg+j, :] = (pt0 + (pt1-pt0)*j/nseg + (pt1-pt0)/(2*nseg))\n",
        "                      p1[iseg+j, :] = (pt0 + (pt1-pt0)*(j+1)/nseg + (pt1-pt0)/(2*nseg))\n",
        "                      p05[iseg+j, :] = (pt0 + (pt1-pt0)*(j+0.5)/nseg)\n",
        "                      r[iseg+j] = sec.diam / 2\n",
        "          iseg += nseg\n",
        "          \n",
        "      self.seg_coords = {'dl': p1 - p0, 'pc': p05, 'r': r}\n",
        "\n",
        "  def __store_segments(self):\n",
        "    self.segments = []\n",
        "    self.sec_id_in_seg = []\n",
        "    nseg = 0\n",
        "    for sec in self.all:\n",
        "        self.sec_id_in_seg.append(nseg)\n",
        "        nseg += sec.nseg\n",
        "        for seg in sec:\n",
        "            self.segments.append(seg)\n",
        "            self.__store_point_processes(seg)\n",
        "    self._nseg = nseg\n",
        "\n",
        "  def __store_synapses_list(self):\n",
        "    '''\n",
        "    store and record synapses from the list from model reduction algorithm\n",
        "    '''\n",
        "    temp_list=[] # generate temp list that has each netcon's synapse obj\n",
        "    for netcon in self.netcons_list:\n",
        "      syn=netcon.syn()\n",
        "      if syn in self.synapses_list:\n",
        "        syn_seg_id=self.segments.index(netcon.syn().get_segment())\n",
        "        if syn in self.segments[syn_seg_id].point_processes():\n",
        "          temp_list.append(syn)\n",
        "        else:\n",
        "          temp_list.append(None)\n",
        "          print(\"Warning: synapse not in designated segment's point processes\")\n",
        "\n",
        "      else:\n",
        "        temp_list.append(None)\n",
        "        print(\"Warning: potentially deleted synapse:\",\"|NetCon obj:\",netcon,\"|Synapse obj:\",syn,\"the NetCon's synapse is not in synapses_list. Check corresponding original cell's NetCon for location, etc.\")\n",
        "    # now use temp list to assign each synapse its netcons\n",
        "    for synapse in self.synapses_list:\n",
        "      synapse_netcons=[]\n",
        "      if synapse in temp_list:\n",
        "        num_netcons=temp_list.count(synapse)\n",
        "        START=0\n",
        "        for i in range(num_netcons):\n",
        "          netcon_id=temp_list.index(synapse,START) #get all the netcon indices that are pointed toward this synapse # use np.where() instead of index() to return multiple indices.\n",
        "          START=netcon_id+1\n",
        "          synapse_netcons.append(self.netcons_list[netcon_id])\n",
        "        self.synapse.append(Listed_Synapse(synapse,synapse_netcons)) #record synapse and add to the list\n",
        "      else:\n",
        "        print('Warning: ', synapse, 'does not have any netcons pointing at it. if synapse is None then deleted synapse may be stored in synapses_list')\n",
        "\n",
        "  def __store_point_processes(self,seg):\n",
        "    for pp in seg.point_processes():\n",
        "        self.injection.append(pp)\n",
        "\n",
        "  def __set_spike_recorder(self, threshold: Optional = None):\n",
        "      if threshold is not None:\n",
        "          self.spike_threshold = threshold\n",
        "      if self.spike_threshold is None:\n",
        "          self.spikes = None\n",
        "      else:\n",
        "          vec = h.Vector()\n",
        "          nc = h.NetCon(self.soma(0.5)._ref_v, None, sec=self.soma)\n",
        "          nc.threshold = self.spike_threshold\n",
        "          nc.record(vec)\n",
        "          self.spikes = vec\n",
        "\n",
        "  def add_injection(self, sec_index, **kwargs):\n",
        "        \"\"\"Add current injection to a section by its index\"\"\"\n",
        "        self.injection.append(CurrentInjection(self, sec_index, **kwargs))\n",
        "\n",
        "  def add_synapse(self, stim: h.NetStim, sec_index: int, **kwargs):\n",
        "        \"\"\"Add synapse to a section by its index\"\"\"\n",
        "        new_syn=Synapse(self, stim, sec_index, **kwargs)\n",
        "        self.netcons_list.append(new_syn.nc)\n",
        "        self.synapse.append(new_syn)\n",
        "\n",
        "  def __generate_sec_coords(self):\n",
        "      '''\n",
        "      Note: need to improve branching so that it is random direction in a quadrant of a sphere rather than x-y plane\n",
        "      takes a cell that has no n3d() coordinates and gives new coordinates\n",
        "      by choosing an arbitrary direction for the subtree to move\n",
        "      '''\n",
        "      section_obj_list= self.all\n",
        "      # print(section_obj_list)\n",
        "      axial=False\n",
        "      parent_sections=[] #list for already seen parent_sections of this type\n",
        "      for sec in section_obj_list:\n",
        "        sec_length=sec.L\n",
        "        if sec is self.soma:\n",
        "          self.sec_angs.append(0)\n",
        "          self.sec_rots.append(0)\n",
        "          # pt0 = [0., -1 * sec.diam, 0.] #does not seem to preserve soma shape , but need to make sure soma children begin at correct 3d coordinate.\n",
        "          # pt1 = [0., 0., 0.]\n",
        "          # sec.pt3dclear()\n",
        "          # sec.pt3dadd(*pt0, sec.diam)\n",
        "          # sec.pt3dadd(*pt1, sec.diam)\n",
        "          if sec.nseg != 1:\n",
        "            print('Changing soma nseg from',sec.nseg,'to 1')\n",
        "            sec.nseg = 1\n",
        "        else:\n",
        "          if sec.parentseg() is not None:\n",
        "            psec=sec.parentseg().sec\n",
        "            if (psec in self.apic) and (psec is not self.apic[0]): # branch\n",
        "              # print('branch')\n",
        "              nbranch = len(psec.children())\n",
        "            else:\n",
        "              nbranch=1\n",
        "          else:\n",
        "            psec=None # may need to provide more implementation in the case of no 3d coords and no parent section.\n",
        "            nbranch=1\n",
        "\n",
        "          # rot = 2 * math.pi/nbranch\n",
        "          rot=np.random.uniform(low=0,high=2*np.pi)# rot can be used to uniformly rotate branches if i=parent_sections.count(psec) and rot = 2 * math.pi/nbranch\n",
        "\n",
        "          i=1 #i can be used to uniformly rotate the sections if rot = 2 * math.pi/nbranch and i=parent_sections.count(psec)\n",
        "          # if nbranch==1:\n",
        "          #   i=1\n",
        "          # else:\n",
        "          #   i=parent_sections.count(psec)\n",
        "\n",
        "          parent_sections.append(psec)\n",
        "          # print(\"sec: \",sec, \"|nbranch: \",nbranch,\"|i: ,\",i,\"|parent_sections:\",parent_sections)\n",
        "          length=sec.L\n",
        "          diameter=sec.diam\n",
        "          fullsecname = sec.name()\n",
        "          # print(fullsecname)\n",
        "          sec_type = fullsecname.split(\".\")[1][:4]\n",
        "          # print(sec_type)\n",
        "          if sec_type == \"apic\":\n",
        "            if sec==self.apic[0]: # trunk\n",
        "              ang=1.570796327\n",
        "            else:\n",
        "              # ang=np.random.uniform(low=0,high=np.pi) #branches\n",
        "              ang=np.random.normal(loc=np.pi/2,scale=0.5) # could add limits to ang (if ang>val:ang=val)\n",
        "          elif sec_type==\"dend\":\n",
        "            # ang=-np.random.uniform(low=0,high=np.pi)\n",
        "            ang=-np.random.normal(loc=np.pi/2,scale=0.5) # could add limits to ang (if ang>val:ang=val)\n",
        "          elif sec_type==\"axon\":\n",
        "            ang=-1.570796327\n",
        "          else:\n",
        "            print(sec,sec_type,' is not apic, dend or axon')\n",
        "            ang=0\n",
        "          if axial == True:\n",
        "            x = 0\n",
        "            y = length*((ang>=0)*2-1)\n",
        "          else:\n",
        "            x = length * math.cos(ang)\n",
        "            y = length * math.sin(ang)\n",
        "          self.sec_angs.append(ang)\n",
        "          self.sec_rots.append(i*rot)\n",
        "          #find starting position #need to update to use parent segment coordinates instead of using first section coordinate\n",
        "          pt0 = [psec.x3d(1), psec.y3d(1), psec.z3d(1)]\n",
        "          pt1 = [0., 0., 0.]\n",
        "          pt1[1] = pt0[1] + y\n",
        "          pt1[0] = pt0[0] + x * math.cos(i * rot)\n",
        "          pt1[2] = pt0[2] + x * math.sin(i * rot)\n",
        "          # print(sec,i*rot)\n",
        "          sec.pt3dclear()\n",
        "          sec.pt3dadd(*pt0, sec.diam)\n",
        "          sec.pt3dadd(*pt1, sec.diam)\n",
        "        if int(sec.L) != int(sec_length):\n",
        "          print('Error: generation of 3D coordinates resulted in change of section length for',sec,'from',sec_length,'to',sec.L)\n",
        "\n",
        "  def __generate_geometry_file(self):\n",
        "    '''\n",
        "    generates geometry file specifying name, pid, ang, radius, length, type\n",
        "    work in progress\n",
        "    '''\n",
        "    df = pd.DataFrame()\n",
        "    ids=[]\n",
        "    names=[]\n",
        "    types=[]\n",
        "    pids=[]\n",
        "    axials=[]\n",
        "    nbranchs=[]\n",
        "    Ls=[]\n",
        "    Rs=[]\n",
        "    angs=self.sec_angs\n",
        "    rots=self.sec_rots\n",
        "    for sec in self.all:\n",
        "      # print(dir(sec))\n",
        "      name=sec.name()\n",
        "      # print(name)\n",
        "      names.append(name)\n",
        "      ids.append(names.index(name))\n",
        "      _,sec_type_withinteger=name.split('.')\n",
        "      sec_type,_=sec_type_withinteger.split('[')\n",
        "      types.append(sec_type)\n",
        "      pseg = sec.parentseg()\n",
        "      if pseg == None:\n",
        "        pids.append(None)\n",
        "      else:\n",
        "        psec=pseg.sec\n",
        "        px3d=psec.x3d\n",
        "        pids.append(int(names.index(psec.name())))\n",
        "      # axials.append('TRUE')\n",
        "      # nbranchs.append(1)\n",
        "      Ls.append(sec.L)\n",
        "      # print(dir(sec))\n",
        "      Rs.append(sec.diam/2)\n",
        "    df['id']=ids\n",
        "    df['name']=names\n",
        "    df['pid']=pids\n",
        "    df['type']=types\n",
        "    df['L']=Ls\n",
        "    df['R']=Rs\n",
        "    try:df['ang']=angs\n",
        "    except:pass\n",
        "    df['rot']=rots\n",
        "    # df['axials']=axials # may need to fix\n",
        "    # df['nbranch']=nbranchs # may need to fix\n",
        "    self.geometry=df\n",
        "\n",
        "  def __convert_sectionlist(self,sectionlist,return_singles=False):\n",
        "    '''\n",
        "    convert nrn sectionlist objects to python list\n",
        "    return_singles set to true will return section instead of [section] for lists with only one section\n",
        "    '''\n",
        "    new_sectionlist=[]\n",
        "    if str(type(sectionlist)) == \"<class 'hoc.HocObject'>\":\n",
        "      for sec in sectionlist:\n",
        "        new_sectionlist.append(sec)\n",
        "    else:\n",
        "      new_sectionlist=sectionlist\n",
        "    if return_singles==True:\n",
        "      if str(type(new_sectionlist))!=\"<class 'nrn.Section'>\":\n",
        "        if len(new_sectionlist)==1:\n",
        "          new_sectionlist=new_sectionlist[0]\n",
        "    return new_sectionlist\n",
        "\n",
        "  def __insert_unused_channels(self):\n",
        "      channels = [('NaTa_t', 'gNaTa_t_NaTa_t', 'gNaTa_tbar'),\n",
        "                  ('Ca_LVAst', 'ica_Ca_LVAst', 'gCa_LVAstbar'),\n",
        "                  ('Ca_HVA', 'ica_Ca_HVA', 'gCa_HVAbar'),\n",
        "                  ('Ih', 'ihcn_Ih', 'gIhbar')]\n",
        "      for channel, attr, conductance in channels:\n",
        "          for sec in self.all:\n",
        "              if not hasattr(sec(0.5), attr):\n",
        "                  sec.insert(channel)\n",
        "                  for seg in sec:\n",
        "                      setattr(getattr(seg, channel), conductance, 0)\n",
        "                  # print(channel, sec) # empty sections\n",
        "\n",
        "  def __setup_recorders(self):\n",
        "      self.gNaTa_T = Recorder(obj_list=self.segments, var_name='gNaTa_t_NaTa_t')\n",
        "      self.ina = Recorder(obj_list=self.segments, var_name='ina_NaTa_t')\n",
        "      self.ical = Recorder(obj_list=self.segments, var_name='ica_Ca_LVAst')\n",
        "      self.icah = Recorder(obj_list=self.segments, var_name='ica_Ca_HVA')\n",
        "      self.ih = Recorder(obj_list=self.segments, var_name='ihcn_Ih')\n",
        "      self.Vm = Recorder(obj_list=self.segments)\n",
        "\n",
        "  def __create_output_folder(self):\n",
        "      nbranches = len(self.apic)-1\n",
        "      nc_count = len(self.netcons_list)\n",
        "      syn_count = len(self.synapses_list)\n",
        "      seg_count = len(self.segments)\n",
        "      \n",
        "\n",
        "      self.output_folder_name = (\n",
        "          str(h.tstop)+\n",
        "          \"outputcontrol_\" +\n",
        "          str(nbranches) + \"nbranch_\" +\n",
        "          str(nc_count) + \"NCs_\" +\n",
        "          str(syn_count) + \"nsyn_\" +\n",
        "          str(seg_count) + \"nseg\"\n",
        "      )\n",
        "\n",
        "      if not os.path.exists(self.output_folder_name):\n",
        "          print('Outputting data to ', self.output_folder_name)\n",
        "          os.makedirs(self.output_folder_name)\n",
        "\n",
        "      return self.output_folder_name\n",
        "\n",
        "  def get_recorder_data(self):\n",
        "      '''\n",
        "      Method for calculating net synaptic currents and getting data after simulation\n",
        "      '''\n",
        "      numTstep = int(h.tstop/h.dt)\n",
        "      i_NMDA_bySeg = [[0] * (numTstep+1)] * len(self.segments)\n",
        "      i_AMPA_bySeg = [[0] * (numTstep+1)] * len(self.segments)\n",
        "      # i_bySeg = [[0] * (numTstep+1)] * len(self.segments)\n",
        "\n",
        "      for synapse in self.synapses_list:\n",
        "          try:\n",
        "              i_NMDA = np.array(synapse.rec_vec.vec_list[1])\n",
        "              i_AMPA = np.array(synapse.rec_vec.vec_list[0])\n",
        "              seg = synapse.get_segment_id()\n",
        "\n",
        "              try:\n",
        "                  i_NMDA_bySeg[seg] = i_NMDA_bySeg[seg] + i_NMDA\n",
        "                  i_AMPA_bySeg[seg] = i_AMPA_bySeg[seg] + i_AMPA\n",
        "              except:\n",
        "                  pass\n",
        "          except:\n",
        "              continue\n",
        "\n",
        "      i_NMDA_df = pd.DataFrame(i_NMDA_bySeg) * 1000\n",
        "      i_AMPA_df = pd.DataFrame(i_AMPA_bySeg) * 1000\n",
        "      \n",
        "\n",
        "      self.data_dict = {}\n",
        "      self.data_dict['spikes']=self.spikes\n",
        "      self.data_dict['ih_data'] = self.ih.as_numpy()\n",
        "      self.data_dict['gNaTa_T_data'] = self.gNaTa_T.as_numpy()\n",
        "      self.data_dict['ina_data'] = self.ina.as_numpy()\n",
        "      self.data_dict['icah_data'] = self.icah.as_numpy()\n",
        "      self.data_dict['ical_data'] = self.ical.as_numpy()\n",
        "      self.data_dict['Vm'] = self.Vm.as_numpy()\n",
        "      self.data_dict['i_NMDA'] = i_NMDA_df\n",
        "      self.data_dict['i_AMPA'] = i_AMPA_df\n",
        "      # self.data_dict['i'] = i_bySeg\n",
        "      self.__create_output_files(self.__create_output_folder())\n",
        "\n",
        "      return self.data_dict\n",
        "\n",
        "  def __create_output_files(self,output_folder_name):\n",
        "      for name, data in self.data_dict.items():\n",
        "        try:\n",
        "          self.__report_data(f\"{output_folder_name}/{name}_report.h5\", data.T)\n",
        "        except:\n",
        "          self.__report_data(f\"{output_folder_name}/{name}_report.h5\", data)\n",
        "\n",
        "  def __report_data(self,reportname, dataname):\n",
        "      try:\n",
        "          os.remove(reportname)\n",
        "      except FileNotFoundError:\n",
        "          pass\n",
        "\n",
        "      with h5py.File(reportname, 'w') as f:\n",
        "          f.create_dataset(\"report/biophysical/data\", data=dataname)\n",
        "\n",
        "  def plot_seg_heatmap(self, seg_df, color_column):\n",
        "      '''\n",
        "      Plots a heatmap of a segment dataframe, using a specified column for color\n",
        "      color_column  :   attribute that is per segment\n",
        "      Can update segments dataframe so that is instead a segment class with the option of saving the dataframe when initializeing the cell?\n",
        "      '''\n",
        "      if isinstance(getattr(self, color_column), list):\n",
        "          color_data = np.concatenate(getattr(self, color_column))\n",
        "      else:\n",
        "          color_data = getattr(self, color_column)\n",
        "\n",
        "      label = color_column.capitalize()\n",
        "      savename = color_column.lower()\n",
        "\n",
        "      plt.figure(figsize=(4,10))\n",
        "      ax = plt.scatter(seg_df[\"Coord X\"], seg_df[\"Coord Y\"],c = color_data, cmap='jet',)\n",
        "      plt.vlines(110,400,500)\n",
        "      plt.text(0,450,'100 um')\n",
        "      plt.hlines(400,110,210)\n",
        "      plt.text(110,350,'100 um')\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      cbar = plt.colorbar()\n",
        "      cbar.ax.set_ylabel(label, rotation=270)\n",
        "\n",
        "      plt.box(False)\n",
        "      plt.savefig(\"/\"+self.output_folder_name+savename+'.svg')\n",
        "\n",
        "  def write_seg_info_to_csv(self):\n",
        "      seg_info=self.__get_segment_info__()\n",
        "      with open(self.output_folder_name+'/seg_info.csv', mode='w') as file:\n",
        "          writer = csv.DictWriter(file, fieldnames=seg_info[0].keys())\n",
        "          writer.writeheader()\n",
        "          for row in seg_info:\n",
        "              writer.writerow(row)\n",
        "\n",
        "  def __get_segment_info__(self):      \n",
        "      seg_info = []\n",
        "      k = 0\n",
        "      j = 0\n",
        "      for sec in self.all:\n",
        "          sec_type = sec.name().split('.')[1][:4]\n",
        "          for i, seg in enumerate(sec):\n",
        "              seg_info.append({\n",
        "                  'seg': seg,\n",
        "                  'seg_id': j,\n",
        "                  'bmtk_id': k,\n",
        "                  'x': seg.x,\n",
        "                  'Type': sec_type,\n",
        "                  'Sec ID': int(sec.name().split('[')[2].split(']')[0]),\n",
        "                  'nseg': seg.sec.nseg,\n",
        "                  'Ra': seg.sec.Ra,\n",
        "                  'seg_L': sec.L/sec.nseg,\n",
        "              })\n",
        "              j += 1\n",
        "          k += 1\n",
        "      return self.__get_parent_segment_ids(seg_info)\n",
        "\n",
        "  def __get_parent_segment_ids(self, seg_info):\n",
        "      for seg in seg_info:\n",
        "          seg['parent_seg_id'] = None\n",
        "      pseg_ids = []\n",
        "      for i, seg in enumerate(seg_info):\n",
        "          idx = int(np.floor(seg['x'] * seg['nseg']))\n",
        "          if idx != 0:\n",
        "              pseg_id = i-1\n",
        "          else:\n",
        "              pseg = seg['seg'].sec.parentseg()\n",
        "              if pseg is None:\n",
        "                  pseg_id = None\n",
        "              else:\n",
        "                  psec = pseg.sec\n",
        "                  nseg = psec.nseg\n",
        "                  pidx = int(np.floor(pseg.x * nseg))\n",
        "                  if pseg.x == 1.:\n",
        "                      pidx -= 1\n",
        "                  try:\n",
        "                      pseg_id = next(idx for idx, info in enumerate(seg_info) if info['seg'] == psec((pidx + .5) / nseg))\n",
        "                  except StopIteration:\n",
        "                      pseg_id = \"Segment not in segments\"\n",
        "              seg_info[i]['parent_seg_id'] = pseg_id\n",
        "          # pseg_ids.append(pseg_id)\n",
        "      return self.__get_segment_elec_dist(seg_info)\n",
        "\n",
        "  def __get_segment_elec_dist(self, seg_info):\n",
        "      for seg in seg_info:\n",
        "          seg['seg_elec_info'] = {}\n",
        "      freqs = {'delta': 1, 'theta': 4, 'alpha': 8, 'beta': 12, 'gamma': 30}\n",
        "\n",
        "      soma_passive_imp = h.Impedance()\n",
        "      soma_active_imp = h.Impedance()\n",
        "      nexus_passive_imp = h.Impedance()\n",
        "      nexus_active_imp = h.Impedance()\n",
        "      try:\n",
        "          soma_passive_imp.loc(self.hobj.soma[0](0.5))\n",
        "          soma_active_imp.loc(self.hobj.soma[0](0.5))\n",
        "      except:\n",
        "          try:\n",
        "              soma_passive_imp.loc(self.soma[0](0.5))\n",
        "              soma_active_imp.loc(self.soma[0](0.5))\n",
        "          except:\n",
        "              try:\n",
        "                  soma_passive_imp.loc(self.soma(0.5))\n",
        "                  soma_active_imp.loc(self.soma(0.5))\n",
        "              except:\n",
        "                  raise AttributeError(\"Could not locate soma for impedance calculation\")\n",
        "      try:\n",
        "          nexus_passive_imp.loc(self.hobj.apic[0](0.99))\n",
        "          nexus_active_imp.loc(self.hobj.apic[0](0.99))\n",
        "      except:\n",
        "          try:\n",
        "              nexus_passive_imp.loc(self.apic[0](0.99))\n",
        "              nexus_active_imp.loc(self.apic[0](0.99))\n",
        "          except:\n",
        "              try:\n",
        "                  nexus_passive_imp.loc(self.apic(0.99))\n",
        "                  nexus_active_imp.loc(self.apic(0.99))\n",
        "              except:\n",
        "                  raise AttributeError(\"Could not locate the nexus for impedance calculation\")\n",
        "\n",
        "      for freq_name, freq_hz in freqs.items():\n",
        "          soma_passive_imp.compute(freq_hz + 1 / 9e9, 0) #passive from soma\n",
        "          soma_active_imp.compute(freq_hz + 1 / 9e9, 1) #active from soma\n",
        "          nexus_passive_imp.compute(freq_hz + 1 / 9e9, 0) #passive from nexus\n",
        "          nexus_active_imp.compute(freq_hz + 1 / 9e9, 1) #active from nexus\n",
        "          for i, seg in enumerate(seg_info):\n",
        "              elec_dist_info = {\n",
        "                  'active_soma': soma_active_imp.ratio(seg['x']),\n",
        "                  'active_nexus': nexus_active_imp.ratio(seg['x']),\n",
        "                  'passive_soma': soma_passive_imp.ratio(seg['x']),\n",
        "                  'passive_nexus': nexus_passive_imp.ratio(seg['x'])\n",
        "              }\n",
        "              seg_info[i]['seg_elec_info'][freq_name] = elec_dist_info\n",
        "      return self.__calculate_netcons_per_seg(seg_info)\n",
        "\n",
        "  def __calculate_netcons_per_seg(self, seg_info):\n",
        "      NetCon_per_seg = [0] * len(seg_info)\n",
        "      inh_NetCon_per_seg = [0] * len(seg_info)\n",
        "      exc_NetCon_per_seg = [0] * len(seg_info)\n",
        "\n",
        "      v_rest = -60 #used to determine exc/inh may adjust or automate\n",
        "      \n",
        "      # calculate number of synapses for each segment (may want to divide by segment length afterward to get synpatic density)\n",
        "      for netcon in self.netcons_list:\n",
        "          syn = netcon.syn()\n",
        "          if syn in self.synapses_list:\n",
        "              syn_seg_id = seg_info.index(next((s for s in seg_info if s['seg'] == syn.get_segment()), None))\n",
        "              seg_dict = seg_info[syn_seg_id]\n",
        "              if syn in seg_dict['seg'].point_processes():\n",
        "                  NetCon_per_seg[syn_seg_id] += 1 # get synapses per segment\n",
        "                  if syn.e > v_rest:\n",
        "                      exc_NetCon_per_seg[syn_seg_id] += 1\n",
        "                  else:\n",
        "                      inh_NetCon_per_seg[syn_seg_id] += 1\n",
        "              else:\n",
        "                  print(\"Warning: synapse not in designated segment's point processes\")\n",
        "          else:\n",
        "              print(\"Warning: potentially deleted synapse:\",\"|NetCon obj:\",netcon,\"|Synapse obj:\",syn,\"the NetCon's synapse is not in synapses_list. Check corresponding original cell's NetCon for location, etc.\")\n",
        "      \n",
        "      for i, seg in enumerate(seg_info):\n",
        "          seg['netcons_per_seg'] = {\n",
        "              'exc': exc_NetCon_per_seg[i],\n",
        "              'inh': inh_NetCon_per_seg[i],\n",
        "              'total': NetCon_per_seg[i]\n",
        "          }\n",
        "          seg['netcon_density_per_seg'] = {\n",
        "              'exc': exc_NetCon_per_seg[i]/seg['seg_L'],\n",
        "              'inh': inh_NetCon_per_seg[i]/seg['seg_L'],\n",
        "              'total': NetCon_per_seg[i]/seg['seg_L']\n",
        "          }\n",
        "      \n",
        "      return seg_info"
      ],
      "metadata": {
        "id": "k-atXqROE6L0"
      },
      "id": "k-atXqROE6L0",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd expand_example"
      ],
      "metadata": {
        "id": "vXTRrX-ILq6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47784fdb-8a38-4ec8-af01-3fff7b518cd5"
      },
      "id": "vXTRrX-ILq6D",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Model_Reduction_Methods/expand_example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the mod files\n",
        "!nrnivmodl mod"
      ],
      "metadata": {
        "id": "Dm-MdGeb6uZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaa06d1-9d1c-4fa8-8ca4-00f0c755d13f"
      },
      "id": "Dm-MdGeb6uZB",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Model_Reduction_Methods/expand_example\n",
            "Mod files: \"mod/mod/CaDynamics_E2.mod\" \"mod/mod/Ca_HVA.mod\" \"mod/mod/Ca_LVAst.mod\" \"mod/mod/epsp.mod\" \"mod/mod/Ih.mod\" \"mod/mod/Im.mod\" \"mod/mod/K_Pst.mod\" \"mod/mod/K_Tst.mod\" \"mod/mod/Nap_Et2.mod\" \"mod/mod/NaTa_t.mod\" \"mod/mod/NaTs2_t.mod\" \"mod/mod/SK_E2.mod\" \"mod/mod/SKv3_1.mod\"\n",
            "\n",
            "Creating 'x86_64' directory for .o files.\n",
            "\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Ca_HVA.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Ca_LVAst.mod\n",
            " -> \u001b[32mCompiling\u001b[0m mod_func.cpp\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/CaDynamics_E2.mod\n",
            "Translating Ca_HVA.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Ca_HVA.c\n",
            "Thread Safe\n",
            "Translating CaDynamics_E2.mod into /content/Model_Reduction_Methods/expand_example/x86_64/CaDynamics_E2.c\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/epsp.mod\n",
            "Thread Safe\n",
            "Translating Ca_LVAst.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Ca_LVAst.c\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Ih.mod\n",
            "Thread Safe\n",
            "Translating epsp.mod into /content/Model_Reduction_Methods/expand_example/x86_64/epsp.c\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Im.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/K_Pst.mod\n",
            "Translating Ih.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Ih.c\n",
            "Translating Im.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Im.c\n",
            "Thread Safe\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/K_Tst.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Nap_Et2.mod\n",
            "Translating K_Pst.mod into /content/Model_Reduction_Methods/expand_example/x86_64/K_Pst.c\n",
            "Thread Safe\n",
            "Translating K_Tst.mod into /content/Model_Reduction_Methods/expand_example/x86_64/K_Tst.c\n",
            "Translating Nap_Et2.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Nap_Et2.c\n",
            "Thread Safe\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/NaTa_t.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/NaTs2_t.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/SK_E2.mod\n",
            "Translating NaTa_t.mod into /content/Model_Reduction_Methods/expand_example/x86_64/NaTa_t.c\n",
            "Translating NaTs2_t.mod into /content/Model_Reduction_Methods/expand_example/x86_64/NaTs2_t.c\n",
            "Thread Safe\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/SKv3_1.mod\n",
            "Translating SK_E2.mod into /content/Model_Reduction_Methods/expand_example/x86_64/SK_E2.c\n",
            " -> \u001b[32mCompiling\u001b[0m CaDynamics_E2.c\n",
            "Thread Safe\n",
            " -> \u001b[32mCompiling\u001b[0m Ca_HVA.c\n",
            "Translating SKv3_1.mod into /content/Model_Reduction_Methods/expand_example/x86_64/SKv3_1.c\n",
            "Thread Safe\n",
            " -> \u001b[32mCompiling\u001b[0m Ca_LVAst.c\n",
            " -> \u001b[32mCompiling\u001b[0m epsp.c\n",
            " -> \u001b[32mCompiling\u001b[0m Ih.c\n",
            " -> \u001b[32mCompiling\u001b[0m Im.c\n",
            " -> \u001b[32mCompiling\u001b[0m K_Pst.c\n",
            " -> \u001b[32mCompiling\u001b[0m K_Tst.c\n",
            " -> \u001b[32mCompiling\u001b[0m Nap_Et2.c\n",
            " -> \u001b[32mCompiling\u001b[0m NaTa_t.c\n",
            " -> \u001b[32mCompiling\u001b[0m NaTs2_t.c\n",
            " -> \u001b[32mCompiling\u001b[0m SK_E2.c\n",
            " -> \u001b[32mCompiling\u001b[0m SKv3_1.c\n",
            " => \u001b[32mLINKING\u001b[0m shared library ./libnrnmech.so\n",
            " => \u001b[32mLINKING\u001b[0m executable ./special LDFLAGS are:    -pthread\n",
            "Successfully created x86_64/special\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a84fbea",
      "metadata": {
        "id": "2a84fbea"
      },
      "source": [
        "## Setup smiulation parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4be7f6fd",
      "metadata": {
        "id": "4be7f6fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7a4f77-95ff-4075-eff7-5d4bfbdf9337"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from neuron import h\n",
        "from scipy import signal\n",
        "from IPython.display import display, clear_output\n",
        "from ipywidgets import interactive_output, HBox, VBox, Label, Layout\n",
        "\n",
        "from __future__ import division\n",
        "from neuron import gui,h\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "h.load_file('stdrun.hoc')\n",
        "# h.nrn_load_dll(paths.COMPILED_LIBRARY_REDUCED_ORDER)  # choose the set of mechanisms\n",
        "h.nrn_load_dll('./x86_64/.libs/libnrnmech.so')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7927a07b",
      "metadata": {
        "id": "7927a07b"
      },
      "source": [
        "### Create a cell with reduced morphology"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "s0gis1f1OdWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9876d0d-f39c-4cda-f9ff-44b4c4bf1ea3"
      },
      "id": "s0gis1f1OdWX",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell1.asc  example_expand.py  L5PCtemplate.hoc  \u001b[0m\u001b[01;34mx86_64\u001b[0m/\n",
            "Cell.hoc   L5PCbiophys3.hoc   \u001b[01;34mmod\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# syn_to_netcon = {} # dictionary mapping netcons to their synapse\n",
        "# for netcon in netcons_list: #fill in dictionary\n",
        "#   syn = netcon.syn() # get the synapse that netcon points to\n",
        "#   if syn in syn_to_netcon:\n",
        "#       syn_to_netcon[syn].append(netcon) #add netcon to existing synapse key\n",
        "#   else:\n",
        "#       syn_to_netcon[syn] = [netcon] #create new synapse key using netcon as an item"
      ],
      "metadata": {
        "id": "xhFzHECCaoqe"
      },
      "id": "xhFzHECCaoqe",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for netcon in syn_to_netcon[synapses_list[0]]:\n",
        "#   print(netcon)"
      ],
      "metadata": {
        "id": "lTzsFC_sbFqD"
      },
      "id": "lTzsFC_sbFqD",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "18a97eeb",
      "metadata": {
        "id": "18a97eeb"
      },
      "outputs": [],
      "source": [
        "\n",
        "h.load_file('L5PCbiophys3.hoc') # load membrane biophysics\n",
        "h.load_file(\"import3d.hoc\") #load 3d morphology\n",
        "\n",
        "\n",
        "# Create a cell object\n",
        "h.load_file('L5PCtemplate.hoc') # load template for generating object\n",
        "complex_cell = h.L5PCtemplate('cell1.asc') # generate object\n",
        "\n",
        "#specify some parameters\n",
        "h.celsius = 37\n",
        "h.v_init = complex_cell.soma[0].e_pas\n",
        "\n",
        "#Add synapses to the complex model\n",
        "synapses_list, netstims_list, netcons_list, randoms_list = [], [], [] ,[]\n",
        "\n",
        "all_segments = [i for j in map(list,list(complex_cell.apical)) for i in j] + [i for j in map(list,list(complex_cell.basal)) for i in j]\n",
        "len_per_segment = np.array([seg.sec.L/seg.sec.nseg for seg in all_segments])\n",
        "rnd = np.random.RandomState(10)\n",
        "for i in range(10000):\n",
        "    seg_for_synapse = rnd.choice(all_segments,   p=len_per_segment/sum(len_per_segment)) #choose a random segment with probability based on the length of segment\n",
        "    synapses_list.append(h.Exp2Syn(seg_for_synapse))\n",
        "    if rnd.uniform()<0.85: # 85% synapses are excitatory\n",
        "        e_syn, tau1, tau2, spike_interval, syn_weight = 0, 0.3, 1.8,  1000/2.5, 0.0016\n",
        "    else: #inhibitory case\n",
        "        e_syn, tau1, tau2, spike_interval, syn_weight = -86, 1,   8,   1000/15.0, 0.0008\n",
        "    #set synaptic varibales\n",
        "    synapses_list[i].e, synapses_list[i].tau1, synapses_list[i].tau2 = e_syn, tau1, tau2\n",
        "    #set netstim variables\n",
        "    netstims_list.append(h.NetStim())\n",
        "    netstims_list[i].interval, netstims_list[i].number, netstims_list[i].start, netstims_list[i].noise = spike_interval, 9e9, 100, 1\n",
        "    #set random\n",
        "    randoms_list.append(h.Random())\n",
        "    randoms_list[i].Random123(i)\n",
        "    randoms_list[i].negexp(1)\n",
        "    netstims_list[i].noiseFromRandom(randoms_list[i])       \n",
        "    #set netcon varibales \n",
        "    netcons_list.append(h.NetCon(netstims_list[i], synapses_list[i] ))\n",
        "    netcons_list[i].delay, netcons_list[i].weight[0] = 0, syn_weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reduce each dendritic subtree to a single cable\n",
        "reduced_cell, synapses_list, netcons_list, txt = subtree_reductor(complex_cell, synapses_list, netcons_list, reduction_frequency=0,return_seg_to_seg=True)"
      ],
      "metadata": {
        "id": "o76VOo9ACebo"
      },
      "id": "o76VOo9ACebo",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #insert unused channels for recorder\n",
        "# for sec in reduced_cell.hoc_model.all:\n",
        "#   if not hasattr(sec(0.5),'gNaTa_t_NaTa_t'):\n",
        "#     sec.insert('NaTa_t')\n",
        "#     for seg in sec:\n",
        "#       seg.NaTa_t.gNaTa_tbar=0\n",
        "#     print(sec)"
      ],
      "metadata": {
        "id": "zUF5QNHj5fzV"
      },
      "id": "zUF5QNHj5fzV",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check synapses_list with netcons_list\n",
        "for netcon in netcons_list:\n",
        "  syn=netcon.syn()\n",
        "  if syn not in synapses_list:\n",
        "    print(syn, netcon)"
      ],
      "metadata": {
        "id": "eAWQMC4tGRFj"
      },
      "id": "eAWQMC4tGRFj",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#expand cables to idealized dendritic trees\n",
        "sections_to_expand = [reduced_cell.hoc_model.apic[0]] # expand apical cylinder\n",
        "furcations_x=[0.289004] #chose using the location of the mapped nexus branching segment\n",
        "nbranches=[4]\n",
        "reduced_dendritic_cell, synapses_list, netcons_list, txt = cable_expander(reduced_cell, sections_to_expand, furcations_x, nbranches, \n",
        "                                                                          synapses_list, netcons_list, reduction_frequency=0,return_seg_to_seg=True)"
      ],
      "metadata": {
        "id": "BQpLyTABCjWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76cbe631-2d41-4ca6-ceaf-1efe9e7ad87d"
      },
      "id": "BQpLyTABCjWT",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-23 09:16:26 Branching the section using (d)3/2 and electrotonic length rule to preserve service area and electrical properties.\n",
            "branch_L: 1081.9141048660235 |branch_diam: 1.475351301247016 |trunk_L: 698.0975964944687 |trunk_diam: 3.7176523208618155\n",
            "2023-04-23 09:16:26 Spreading synapses onto branches\n",
            "Exp2Syn ['e']\n",
            "2023-04-23 09:16:26 duplicating branch 1 synapses onto the other branches and randomly distributing Netcons\n",
            "number of reduced synapses before duplicating synapses to branches: 92\n",
            "[model[1].apic[1], model[1].apic[2], model[1].apic[3], model[1].apic[4]]\n",
            "[Exp2Syn[21], Exp2Syn[3]]\n",
            "Exp2Syn[10000]\n",
            "Exp2Syn[10001]\n",
            "Exp2Syn[10002]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10003]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10004]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10005]\n",
            "[Exp2Syn[42], Exp2Syn[23]]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10006]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10007]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10008]\n",
            "Exp2Syn[10009]\n",
            "Exp2Syn[10010]\n",
            "Exp2Syn[10011]\n",
            "[Exp2Syn[105], Exp2Syn[24]]\n",
            "Exp2Syn[10012]\n",
            "Exp2Syn[10013]\n",
            "Exp2Syn[10014]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10015]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10016]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10017]\n",
            "[Exp2Syn[596], Exp2Syn[34]]\n",
            "Exp2Syn[10018]\n",
            "Exp2Syn[10019]\n",
            "Exp2Syn[10020]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10021]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10022]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10023]\n",
            "[Exp2Syn[1014], Exp2Syn[251]]\n",
            "Exp2Syn[10024]\n",
            "Exp2Syn[10025]\n",
            "Exp2Syn[10026]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10027]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10028]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10029]\n",
            "[Exp2Syn[101], Exp2Syn[25]]\n",
            "Exp2Syn[10030]\n",
            "Exp2Syn[10031]\n",
            "Exp2Syn[10032]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10033]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10034]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10035]\n",
            "[Exp2Syn[1857], Exp2Syn[47]]\n",
            "Exp2Syn[10036]\n",
            "Exp2Syn[10037]\n",
            "Exp2Syn[10038]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10039]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10040]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10041]\n",
            "[Exp2Syn[1440], Exp2Syn[37]]\n",
            "Exp2Syn[10042]\n",
            "Exp2Syn[10043]\n",
            "Exp2Syn[10044]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10045]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10046]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10047]\n",
            "[Exp2Syn[131], Exp2Syn[22]]\n",
            "Exp2Syn[10048]\n",
            "Exp2Syn[10049]\n",
            "Exp2Syn[10050]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10051]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10052]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10053]\n",
            "[Exp2Syn[27], Exp2Syn[11]]\n",
            "Exp2Syn[10054]\n",
            "Exp2Syn[10055]\n",
            "Exp2Syn[10056]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10057]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10058]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10059]\n",
            "[Exp2Syn[346], Exp2Syn[12]]\n",
            "Exp2Syn[10060]\n",
            "Exp2Syn[10061]\n",
            "Exp2Syn[10062]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10063]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10064]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10065]\n",
            "[Exp2Syn[1204], Exp2Syn[647]]\n",
            "Exp2Syn[10066]\n",
            "Exp2Syn[10067]\n",
            "Exp2Syn[10068]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10069]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10070]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10071]\n",
            "[Exp2Syn[712], Exp2Syn[93]]\n",
            "Exp2Syn[10072]\n",
            "Exp2Syn[10073]\n",
            "Exp2Syn[10074]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10075]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10076]\n",
            "consider excluding PP_params: e\n",
            "Exp2Syn[10077]\n",
            "2023-04-23 09:16:26 Finish YAY\n",
            "number of reduced synapses after duplicating synapses to branches: 170\n",
            "2023-04-23 09:16:26 Mapping segments\n",
            "2023-04-23 09:16:26 Mapping mechanisms\n",
            "2023-04-23 09:16:26 Deleting original model sections\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check synapses_list with netcons_list\n",
        "for netcon in netcons_list:\n",
        "  syn=netcon.syn()\n",
        "  if syn not in synapses_list:\n",
        "    print(syn, netcon)"
      ],
      "metadata": {
        "id": "HlRaujdIGUqF"
      },
      "id": "HlRaujdIGUqF",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # try branching the branches - does not work yet\n",
        "# sections_to_expand = [reduced_dendritic_cell.apic[1],reduced_dendritic_cell.apic[2],reduced_dendritic_cell.apic[3],reduced_dendritic_cell.apic[4]]\n",
        "# furcations_x=[0.50,0.50,0.50,0.50]\n",
        "# nbranches=[4,4,4,4]\n",
        "# reduced_dendritic_cell, synapses_list, netcons_list, txt = cable_expander(reduced_dendritic_cell, sections_to_expand, furcations_x, nbranches, \n",
        "#                                                                           synapses_list, netcons_list, reduction_frequency=0,return_seg_to_seg=True)"
      ],
      "metadata": {
        "id": "1Ap8Gp-mCm-4"
      },
      "id": "1Ap8Gp-mCm-4",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check seg mapping\n",
        "# for i in txt:\n",
        "#   print(i,\"was mapped to\",txt[i])"
      ],
      "metadata": {
        "id": "LPXtOiycci5T"
      },
      "id": "LPXtOiycci5T",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use defined cell_model python class for generating 3d coordinates, recording ECP, 'book-keeping' etc...\n",
        "import random\n",
        "random.seed(2)\n",
        "cell = cell_model(reduced_dendritic_cell,synapses_list=synapses_list,netcons_list=netcons_list,spike_threshold = 10)\n",
        "# cell._nbranch=4"
      ],
      "metadata": {
        "id": "NndM90Z9SrU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d574e9bf-ecb0-47ad-cad9-bb899a4c2b31"
      },
      "id": "NndM90Z9SrU7",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning:  Exp2Syn[1204] does not have any netcons pointing at it. if synapse is None then deleted synapse may be stored in synapses_list\n",
            "Warning:  Exp2Syn[10026] does not have any netcons pointing at it. if synapse is None then deleted synapse may be stored in synapses_list\n",
            "Warning:  Exp2Syn[10066] does not have any netcons pointing at it. if synapse is None then deleted synapse may be stored in synapses_list\n",
            "Warning:  Exp2Syn[10067] does not have any netcons pointing at it. if synapse is None then deleted synapse may be stored in synapses_list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warning message comes from random chance that synapse did not recieve a netcon after redistributing from single cable to equivalent branches."
      ],
      "metadata": {
        "id": "mMIwx7oPfaZ2"
      },
      "id": "mMIwx7oPfaZ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regenerate Original Cell"
      ],
      "metadata": {
        "id": "ei45mgNULTNc"
      },
      "id": "ei45mgNULTNc"
    },
    {
      "cell_type": "code",
      "source": [
        "orig_cell = h.L5PCtemplate('cell1.asc') # generate object\n",
        "\n",
        "#Add synapses to the complex model\n",
        "synapses_list, netstims_list, netcons_list, randoms_list = [], [], [] ,[]\n",
        "\n",
        "all_segments = [i for j in map(list,list(orig_cell.apical)) for i in j] + [i for j in map(list,list(orig_cell.basal)) for i in j]\n",
        "len_per_segment = np.array([seg.sec.L/seg.sec.nseg for seg in all_segments])\n",
        "rnd = np.random.RandomState(10)\n",
        "for i in range(10000):\n",
        "    seg_for_synapse = rnd.choice(all_segments,   p=len_per_segment/sum(len_per_segment)) #choose a random segment with probability based on the length of segment\n",
        "    synapses_list.append(h.Exp2Syn(seg_for_synapse))\n",
        "    if rnd.uniform()<0.85: # 85% synapses are excitatory\n",
        "        e_syn, tau1, tau2, spike_interval, syn_weight = 0, 0.3, 1.8,  1000/2.5, 0.0016\n",
        "    else: #inhibitory case\n",
        "        e_syn, tau1, tau2, spike_interval, syn_weight = -86, 1,   8,   1000/15.0, 0.0008\n",
        "    #set synaptic varibales\n",
        "    synapses_list[i].e, synapses_list[i].tau1, synapses_list[i].tau2 = e_syn, tau1, tau2\n",
        "    #set netstim variables\n",
        "    netstims_list.append(h.NetStim())\n",
        "    netstims_list[i].interval, netstims_list[i].number, netstims_list[i].start, netstims_list[i].noise = spike_interval, 9e9, 100, 1\n",
        "    #set random\n",
        "    randoms_list.append(h.Random())\n",
        "    randoms_list[i].Random123(i)\n",
        "    randoms_list[i].negexp(1)\n",
        "    netstims_list[i].noiseFromRandom(randoms_list[i])       \n",
        "    #set netcon varibales \n",
        "    netcons_list.append(h.NetCon(netstims_list[i], synapses_list[i] ))\n",
        "    netcons_list[i].delay, netcons_list[i].weight[0] = 0, syn_weight"
      ],
      "metadata": {
        "id": "R7Y2l7W4LS1E"
      },
      "id": "R7Y2l7W4LS1E",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_cell = cell_model(model=orig_cell,\n",
        "                           synapses_list=synapses_list,netcons_list=netcons_list,\n",
        "                           gen_3d=False,gen_geom_csv=False,\n",
        "                           spike_threshold = 10)"
      ],
      "metadata": {
        "id": "64gxUrUiSSxN"
      },
      "id": "64gxUrUiSSxN",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#may be able to delete\n",
        "def get_syn_to_netcons(netcons_list):\n",
        "    syn_to_netcon = {} # dictionary mapping netcons to their synapse\n",
        "    for netcon in netcons_list: #fill in dictionary\n",
        "      syn = netcon.syn() # get the synapse that netcon points to\n",
        "      if syn in syn_to_netcon:\n",
        "          syn_to_netcon[syn].append(netcon) #add netcon to existing synapse key\n",
        "      else:\n",
        "          syn_to_netcon[syn] = [netcon] #create new synapse key using netcon as an item\n",
        "    return syn_to_netcon\n",
        "\n",
        "syn_to_netcon=get_syn_to_netcons(netcons_list)"
      ],
      "metadata": {
        "id": "1V-HNkx5fIbK"
      },
      "id": "1V-HNkx5fIbK",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #get expanded segments dataframe\n",
        "# make_reduced_seg_df(cell,\"segments_expanded.csv\") #need to improve make_reduced_seg_df\n",
        "# expanded_segments_df=pd.read_csv(\"segments_expanded.csv\")\n",
        "\n",
        "# #get reduced segments dataframe\n",
        "# make_reduced_seg_df(original_cell,\"segments_original.csv\") #need to improve make_reduced_seg_df\n",
        "# original_segments_df=pd.read_csv(original_cell\"segments_original.csv\")\n",
        "\n",
        "# # change to complex cell\n",
        "# # make_reduced_seg_df(cell,\"segments_expanded.csv\") #need to improve make_reduced_seg_df\n",
        "# # expanded_segments_df=pd.read_csv(\"segments_expanded.csv\")\n",
        "# # plot_morphology(expanded_segments_df,\"expanded_morphology.svg\")"
      ],
      "metadata": {
        "id": "JjM51nSARop9"
      },
      "id": "JjM51nSARop9",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(cell)"
      ],
      "metadata": {
        "id": "akYHQOGDKiW5"
      },
      "id": "akYHQOGDKiW5",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount ECP dir may not be necessary"
      ],
      "metadata": {
        "id": "FuJox5mRCkMJ"
      },
      "id": "FuJox5mRCkMJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# #mount ECP dir # may only need this for ECP notebook\n",
        "# import os\n",
        "\n",
        "# RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "# if RunningInCOLAB:\n",
        "#     !pip install neuron==8.0.0 &> /dev/null\n",
        "#     os.chdir('/content')\n",
        "#     if not os.path.isdir('Stylized-Single-Cell-and-Extracellular-Potential'):\n",
        "#         !git clone https://github.com/chenziao/Stylized-Single-Cell-and-Extracellular-Potential.git &> /dev/null \n",
        "#     os.chdir('Stylized-Single-Cell-and-Extracellular-Potential')\n",
        "#     %ls"
      ],
      "metadata": {
        "id": "D6vyubrwOvA_"
      },
      "id": "D6vyubrwOvA_",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(original_cell)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Dy-nYIHzRx",
        "outputId": "2dee6838-7449-4b75-8858-bdd429e5e018"
      },
      "id": "q5Dy-nYIHzRx",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vm',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__get_segment_info__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_cell_model__calc_seg_coords',\n",
              " '_cell_model__calculate_netcons_per_seg',\n",
              " '_cell_model__convert_sectionlist',\n",
              " '_cell_model__create_output_files',\n",
              " '_cell_model__create_output_folder',\n",
              " '_cell_model__generate_geometry_file',\n",
              " '_cell_model__generate_sec_coords',\n",
              " '_cell_model__get_parent_segment_ids',\n",
              " '_cell_model__get_segment_elec_dist',\n",
              " '_cell_model__insert_unused_channels',\n",
              " '_cell_model__report_data',\n",
              " '_cell_model__set_spike_recorder',\n",
              " '_cell_model__setup_recorders',\n",
              " '_cell_model__store_point_processes',\n",
              " '_cell_model__store_segments',\n",
              " '_cell_model__store_synapses_list',\n",
              " '_nseg',\n",
              " 'add_injection',\n",
              " 'add_synapse',\n",
              " 'all',\n",
              " 'apic',\n",
              " 'axon',\n",
              " 'dend',\n",
              " 'gNaTa_T',\n",
              " 'get_recorder_data',\n",
              " 'grp_ids',\n",
              " 'icah',\n",
              " 'ical',\n",
              " 'ih',\n",
              " 'ina',\n",
              " 'injection',\n",
              " 'netcons_list',\n",
              " 'plot_seg_heatmap',\n",
              " 'sec_angs',\n",
              " 'sec_id_in_seg',\n",
              " 'sec_id_lookup',\n",
              " 'sec_rots',\n",
              " 'seg_coords',\n",
              " 'segments',\n",
              " 'soma',\n",
              " 'spike_threshold',\n",
              " 'spikes',\n",
              " 'synapse',\n",
              " 'synapses_list',\n",
              " 'write_seg_info_to_csv']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# original_cell.write_seg_info_to_csv()\n",
        "# original_segments_df=pd.read_csv('/'+original_cell.output_folder_name+'/seg_info.csv')"
      ],
      "metadata": {
        "id": "NkDrwG1qBTjQ"
      },
      "id": "NkDrwG1qBTjQ",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell.__write_seg_info_to_csv()\n",
        "# expanded_segments_df=pd.read_csv('/'+cell.output_folder_name+'/seg_info.csv')"
      ],
      "metadata": {
        "id": "Q6dejn_mBT5-"
      },
      "id": "Q6dejn_mBT5-",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(tstop=5000,dt=0.1):\n",
        "    '''\n",
        "    Run simulation and print the runtime\n",
        "    tstop :  ms simulation duration\n",
        "    '''\n",
        "    # Define simulation parameters\n",
        "    h.tstop = tstop\n",
        "    h.dt = dt\n",
        "    h.steps_per_ms = 1/h.dt\n",
        "\n",
        "    # Define simulation start time\n",
        "    timestart = time.time()\n",
        "\n",
        "    # Run simulation\n",
        "    h.run()\n",
        "\n",
        "    # Define simulation stop time\n",
        "    timestop = time.time()\n",
        "\n",
        "    # Calculate and print the runtime\n",
        "    elapsedtime = timestop - timestart\n",
        "    simtime = tstop/1000  # convert from ms to s\n",
        "    print(f\"It took {round(elapsedtime)} sec to run a {simtime} sec simulation.\")\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "vyo3zUnohkN5"
      },
      "id": "vyo3zUnohkN5",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation(tstop=5000)\n",
        "# #Run simulation\n",
        "\n",
        "# timestart=time.time()\n",
        "# h.run()\n",
        "# timestop=time.time()\n",
        "# t = h.t # was t=h.t() but 'float' object not callable\n",
        "# elapsedtime=timestop-timestart\n",
        "# simtime=tstop/1000 #convert from ms to s\n",
        "# # totaltime= elapsedtime+elapseddeftime\n",
        "# print('It took',round(elapsedtime),'sec to run a',simtime,'sec simulation.')\n",
        "# # print('The total runtime was',round(totaltime),'sec')"
      ],
      "metadata": {
        "id": "U5azilotOvGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e783fc31-58b8-4aae-9699-1258df614824"
      },
      "id": "U5azilotOvGY",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 209 sec to run a 5.0 sec simulation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import h5py\n",
        "# def createsegtracereport(reportname,dataname):\n",
        "#   try:\n",
        "#     os.remove(reportname) # reportname was string \" \"\n",
        "#   except:\n",
        "#     x = 1\n",
        "\n",
        "#   f = h5py.File(reportname,'w') #create a file in the w (write) mode #reportname was string ' '\n",
        "#   v = f.create_dataset(\"report/biophysical/data\", data = dataname)\n",
        "#   f.close()"
      ],
      "metadata": {
        "id": "hhPWsoCPeg4l"
      },
      "id": "hhPWsoCPeg4l",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_net_synaptic_currents(cell):\n",
        "#     numTstep = int(h.tstop/h.dt)\n",
        "#     i_NMDA_bySeg = [[0] * (numTstep+1)] * len(cell.segments)\n",
        "#     i_AMPA_bySeg = [[0] * (numTstep+1)] * len(cell.segments)\n",
        "#     i_bySeg = [[0] * (numTstep+1)] * len(cell.segments)\n",
        "\n",
        "#     for synapse in cell.synapses_list:\n",
        "#         try:\n",
        "#             i_NMDA = np.array(synapse.rec_vec.vec_list[1])\n",
        "#             i_AMPA = np.array(synapse.rec_vec.vec_list[0])\n",
        "#             seg = synapse.get_segment_id()\n",
        "\n",
        "#             try:\n",
        "#                 i_NMDA_bySeg[seg] = i_NMDA_bySeg[seg] + i_NMDA\n",
        "#                 i_AMPA_bySeg[seg] = i_AMPA_bySeg[seg] + i_AMPA\n",
        "#                 i_bySeg[seg] = i_bySeg[seg] + i_NMDA + i_AMPA\n",
        "#             except:\n",
        "#                 pass\n",
        "#         except:\n",
        "#             pass\n",
        "    \n",
        "#     return i_NMDA_bySeg, i_AMPA_bySeg, i_bySeg"
      ],
      "metadata": {
        "id": "scLK50hZjZmZ"
      },
      "id": "scLK50hZjZmZ",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate inmda\n",
        "#since synapse list has combined synapses for computational efficiency, we must use NetCon list/unique spike trains to determine how many synapses were mapped\n",
        "\n",
        "# v_rest=-60 #choose v_rest for categorizing inh/exc synapses\n",
        "\n",
        "# NetCon_per_seg=[0]*len(cell.segments)\n",
        "# inh_NetCon_per_seg=[0]*len(cell.segments)\n",
        "# exc_NetCon_per_seg=[0]*len(cell.segments)\n",
        "\n",
        "# #calculate number of synapses for each segment (may want to divide by segment length afterward to get synpatic density)\n",
        "# for netcon in cell.netcons_list:\n",
        "#   syn=netcon.syn()\n",
        "#   if syn in synapses_list:\n",
        "#     syn_seg_id=cell.segments.index(netcon.syn().get_segment())\n",
        "#     if syn in cell.segments[syn_seg_id].point_processes():\n",
        "#       NetCon_per_seg[syn_seg_id]+=1 # get synapses per segment\n",
        "#       # NetCon_per_seg[syn_seg_id].append(netcon) # possible implementation if needing objects per segment\n",
        "#       if syn.e > v_rest:\n",
        "#         exc_NetCon_per_seg[syn_seg_id]+=1\n",
        "#         # exc_NetCon_per_seg[syn_seg_id].append(netcon)# possible implementation if needing objects per segment\n",
        "#       else:\n",
        "#         inh_NetCon_per_seg[syn_seg_id]+=1\n",
        "#         # inh_NetCon_per_seg[syn_seg_id].append(netcon)# possible implementation if needing objects per segment\n",
        "#     else:\n",
        "#       print(\"Warning: synapse not in designated segment's point processes\")\n",
        "\n",
        "#   else:\n",
        "#     print(\"Warning: potentially deleted synapse:\",\"|NetCon obj:\",netcon,\"|Synapse obj:\",syn,\"the NetCon's synapse is not in synapses_list. Check corresponding original cell's NetCon for location, etc.\")\n",
        "\n",
        "# numTstep = int(h.tstop/h.dt)\n",
        "# i_NMDA_bySeg= [[0] * (numTstep+1) ] * len(cell.segments) # need to implement inmda recording\n",
        "\n",
        "# #extract inmda from each segment # can be adjusted for gaba synapses or alpha synapses # only paired nmda/ampa synapses have vec_list[1]\n",
        "# for synapse in cell.synapse:\n",
        "#   try:\n",
        "#     i_NMDA = np.array(synapse.rec_vec.vec_list[1])            #current = numpy array of NEURON Vector of current NMDA current at synapse j  \n",
        "#     seg = synapse.get_segment_id()                            #seg = the segment in which synapse j is located \n",
        "#     #print('first try')\n",
        "    \n",
        "#     try:\n",
        "#       i_NMDA_bySeg[seg] = i_NMDA_bySeg[seg] + i_NMDA    \n",
        "#       #print('second try')                                           #Sum current over each segment\n",
        "#     except: \n",
        "#       pass                                                                   #Except needed as some synpases do not have NMDA currrent and throw an error when called\n",
        "#   except:\n",
        "#     pass"
      ],
      "metadata": {
        "id": "cbYSa3y5cKCY"
      },
      "id": "cbYSa3y5cKCY",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_netcons_per_seg(self):\n",
        "#   NetCon_per_seg=[0]*len(self.segments)\n",
        "#   inh_NetCon_per_seg=[0]*len(self.segments)\n",
        "#   exc_NetCon_per_seg=[0]*len(self.segments)\n",
        "\n",
        "#   #calculate number of synapses for each segment (may want to divide by segment length afterward to get synpatic density)\n",
        "#   for netcon in self.netcons_list:\n",
        "#     syn=netcon.syn()\n",
        "#     if syn in synapses_list:\n",
        "#       syn_seg_id=self.segments.index(netcon.syn().get_segment())\n",
        "#       if syn in self.segments[syn_seg_id].point_processes():\n",
        "#         NetCon_per_seg[syn_seg_id]+=1 # get synapses per segment\n",
        "#         # NetCon_per_seg[syn_seg_id].append(netcon) # possible implementation if needing objects per segment\n",
        "#         if syn.e > v_rest:\n",
        "#           exc_NetCon_per_seg[syn_seg_id]+=1\n",
        "#           # exc_NetCon_per_seg[syn_seg_id].append(netcon)# possible implementation if needing objects per segment\n",
        "#         else:\n",
        "#           inh_NetCon_per_seg[syn_seg_id]+=1\n",
        "#           # inh_NetCon_per_seg[syn_seg_id].append(netcon)# possible implementation if needing objects per segment\n",
        "#       else:\n",
        "#         print(\"Warning: synapse not in designated segment's point processes\")\n",
        "\n",
        "#     else:\n",
        "#       print(\"Warning: potentially deleted synapse:\",\"|NetCon obj:\",netcon,\"|Synapse obj:\",syn,\"the NetCon's synapse is not in synapses_list. Check corresponding original cell's NetCon for location, etc.\")\n",
        "#   return NetCon_per_seg,inh_NetCon_per_seg,exc_NetCon_per_seg"
      ],
      "metadata": {
        "id": "36R_7mqvMjJC"
      },
      "id": "36R_7mqvMjJC",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expanded_segments_df[\"exc_NetCons\"]=cell.exc_NetCon_per_seg\n",
        "# expanded_segments_df[\"inh_NetCons\"]=cell.inh_NetCon_per_seg\n",
        "# expanded_segments_df[\"inh_NetCon_density\"]=cell.expanded_segments_df[\"inh_NetCons\"]/expanded_segments_df[\"Seg_L\"]\n",
        "# expanded_segments_df[\"exc_NetCon_density\"]=cell.expanded_segments_df[\"exc_NetCons\"]/expanded_segments_df[\"Seg_L\"]\n",
        "# expanded_segments_df.to_csv(\"segments_expanded.csv\") #update saved data to include above columns. \n",
        "# #Can instead include above columns and calculation in \"make_reduced_seg_df()\""
      ],
      "metadata": {
        "id": "_ip03H02CFV1"
      },
      "id": "_ip03H02CFV1",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #subset dataframe to ignore certain segments and see synaptic density of branches\n",
        "# ignore=[]\n",
        "# for seg in cell.apic[0]:#ignore trunk\n",
        "#   ind=cell.segments.index(seg)\n",
        "#   ignore.append(ind)\n",
        "# df=expanded_segments_df[~expanded_segments_df.index.isin(ignore)]"
      ],
      "metadata": {
        "id": "Q2WAz7JY73CJ"
      },
      "id": "Q2WAz7JY73CJ",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #can stow this function away\n",
        "# def plot_seg_heatmap(seg_df,label=str,savename=str,color_column=str):\n",
        "#   '''color_column: name of the column that you want to use as the parameter for heatmap color'''\n",
        "#   plt.figure(figsize=(4,10))\n",
        "#   ax = plt.scatter(seg_df[\"Coord X\"], seg_df[\"Coord Y\"],c = seg_df[color_column],cmap='jet',)\n",
        "#   plt.vlines(110,400,500)\n",
        "#   plt.text(0,450,'100 um')\n",
        "#   plt.hlines(400,110,210)\n",
        "#   plt.text(110,350,'100 um')\n",
        "#   plt.xticks([])\n",
        "#   plt.yticks([])\n",
        "#   cbar = plt.colorbar()\n",
        "#   cbar.ax.set_ylabel(label, rotation=270)\n",
        "\n",
        "#   plt.box(False)\n",
        "#   plt.savefig(savename+'.svg')"
      ],
      "metadata": {
        "id": "M5S7mlKrDS4z"
      },
      "id": "M5S7mlKrDS4z",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HjKSPRsXHgTy"
      },
      "id": "HjKSPRsXHgTy"
    },
    {
      "cell_type": "code",
      "source": [
        "# output_folder_name=(\n",
        "#     \"outputcontrol_\"+\n",
        "#     str(nbranches[0])+\"nbranch_\"+\n",
        "#     str(int(len(cell.netcons_list)))+\"NCs_\"\n",
        "#     +str(int(len(cell.synapses_list)))+\"nsyn_\"+\n",
        "#     str(int(len(cell.segments)))+\"nseg\"\n",
        "#                                         )#+modelname  #include model name in output foler name (ex. original cell, reduced cell, expanded cell)\n",
        "# #create output folder\n",
        "# import os\n",
        "# if not os.path.exists(output_folder_name):\n",
        "#    os.makedirs(output_folder_name)\n",
        "\n",
        "# print(output_folder_name)\n",
        "# # os.chdir(output_folder_name)"
      ],
      "metadata": {
        "id": "VrOZ-cMgHXCC"
      },
      "id": "VrOZ-cMgHXCC",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_output_folder(cell):\n",
        "#     nbranches = len(cell.apic)-1\n",
        "#     nc_count = len(cell.netcons_list)\n",
        "#     syn_count = len(cell.synapses_list)\n",
        "#     seg_count = len(cell.segments)\n",
        "    \n",
        "\n",
        "#     output_folder_name = (\n",
        "#         str(h.tstop)+\n",
        "#         \"outputcontrol_\" +\n",
        "#         str(nbranches) + \"nbranch_\" +\n",
        "#         str(nc_count) + \"NCs_\" +\n",
        "#         str(syn_count) + \"nsyn_\" +\n",
        "#         str(seg_count) + \"nseg\"\n",
        "#     )\n",
        "\n",
        "#     if not os.path.exists(output_folder_name):\n",
        "#         os.makedirs(output_folder_name)\n",
        "\n",
        "#     return output_folder_name"
      ],
      "metadata": {
        "id": "iM3WWzrEd43n"
      },
      "id": "iM3WWzrEd43n",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_morphology(seg_df, savename):\n",
        "    plt.figure(figsize=(4, 10))\n",
        "    for i in seg_df[seg_df.type == 'apic']['Sec ID'].unique():\n",
        "        plt.plot(\n",
        "            seg_df[(seg_df.type == 'apic') & (seg_df['Sec ID'] == i)]['Coord X'],\n",
        "            seg_df[(seg_df.type == 'apic') & (seg_df['Sec ID'] == i)]['Coord Y'],\n",
        "            color='k',\n",
        "            linewidth=2 * seg_df[(seg_df.type == 'apic') & (seg_df['Sec ID'] == i)]['Section_diam'].unique()\n",
        "        )\n",
        "    for i in seg_df[seg_df.type == 'dend']['Sec ID'].unique():\n",
        "        plt.plot(\n",
        "            seg_df[(seg_df.type == 'dend') & (seg_df['Sec ID'] == i)]['Coord X'],\n",
        "            seg_df[(seg_df.type == 'dend') & (seg_df['Sec ID'] == i)]['Coord Y'],\n",
        "            color='k',\n",
        "            linewidth=2 * seg_df[(seg_df.type == 'dend') & (seg_df['Sec ID'] == i)]['Section_diam'].unique()\n",
        "        )\n",
        "\n",
        "    # save the figure\n",
        "    plt.axis('off')\n",
        "    plt.savefig(savename, bbox_inches='tight')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "03YsbY2gnDtb"
      },
      "id": "03YsbY2gnDtb",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_cell.get_recorder_data() #also creates directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXxl9eKDdGNt",
        "outputId": "8a01eb7d-eee9-4505-f157-3a1a289b8e55"
      },
      "id": "SXxl9eKDdGNt",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputting data to  5000.0outputcontrol_108nbranch_10000NCs_10000nsyn_642nseg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spikes': Vector[5455],\n",
              " 'ih_data': array([[-1.11185907e-03, -1.11185907e-03, -1.10991652e-03, ...,\n",
              "         -3.18233396e-05, -3.17763926e-05, -3.17166831e-05],\n",
              "        [-1.11185907e-03, -1.11185907e-03, -1.11010559e-03, ...,\n",
              "         -3.25805061e-05, -3.25803868e-05, -3.25761445e-05],\n",
              "        [-1.11185907e-03, -1.11185907e-03, -1.11027407e-03, ...,\n",
              "         -3.35719760e-05, -3.36590309e-05, -3.37550970e-05],\n",
              "        ...,\n",
              "        [-2.28742766e-03, -2.28742766e-03, -2.28187396e-03, ...,\n",
              "         -6.81899692e-05, -6.80538974e-05, -6.79403582e-05],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00]]),\n",
              " 'gNaTa_T_data': array([[3.23378406e-11, 3.23378406e-11, 3.34312556e-11, ...,\n",
              "         6.89000734e-06, 7.00333674e-06, 7.13415041e-06],\n",
              "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "        ...,\n",
              "        [3.37645100e-13, 3.37645100e-13, 3.53617253e-13, ...,\n",
              "         2.00211150e-08, 2.08853120e-08, 2.17126232e-08],\n",
              "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]),\n",
              " 'ina_data': array([[-4.52729768e-09, -4.52729768e-09, -4.67776220e-09, ...,\n",
              "         -7.81680216e-04, -7.94313292e-04, -8.08870657e-04],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        ...,\n",
              "        [-4.72703140e-11, -4.72703140e-11, -4.94679975e-11, ...,\n",
              "         -2.33369557e-06, -2.43273805e-06, -2.52752295e-06],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00]]),\n",
              " 'icah_data': array([[-1.77836898e-15, -1.77836898e-15, -1.79521518e-15, ...,\n",
              "         -1.04465935e-08, -1.03721437e-08, -1.03114316e-08],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        ...,\n",
              "        [-9.94954419e-17, -9.94954419e-17, -1.00807526e-16, ...,\n",
              "         -1.38377818e-10, -1.34837661e-10, -1.31803179e-10],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00]]),\n",
              " 'ical_data': array([[-2.29384682e-08, -2.29384682e-08, -2.29369079e-08, ...,\n",
              "         -4.27269990e-06, -4.26816600e-06, -4.26402066e-06],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        ...,\n",
              "        [-1.25058121e-09, -1.25058121e-09, -1.25042699e-09, ...,\n",
              "         -5.85797353e-07, -5.78917186e-07, -5.72160637e-07],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
              "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00]]),\n",
              " 'Vm': array([[-90.        , -89.92182217, -89.84368898, ..., -63.41926296,\n",
              "         -63.38009571, -63.33624268],\n",
              "        [-90.        , -89.92943149, -89.85580251, ..., -63.47216038,\n",
              "         -63.46594852, -63.45749116],\n",
              "        [-90.        , -89.93621178, -89.86875094, ..., -63.35085447,\n",
              "         -63.40195475, -63.45528732],\n",
              "        ...,\n",
              "        [-90.        , -89.89135746, -89.78692816, ..., -66.48080968,\n",
              "         -66.40799583, -66.34424848],\n",
              "        [-90.        , -89.94047014, -89.86841099, ..., -63.46148274,\n",
              "         -63.42425412, -63.38218447],\n",
              "        [-90.        , -89.95626567, -89.89175976, ..., -63.50357096,\n",
              "         -63.46807501, -63.42778012]]),\n",
              " 'i_NMDA':      0      1      2      3      4      5      6      7      8      9      \\\n",
              " 0        0      0      0      0      0      0      0      0      0      0   \n",
              " 1        0      0      0      0      0      0      0      0      0      0   \n",
              " 2        0      0      0      0      0      0      0      0      0      0   \n",
              " 3        0      0      0      0      0      0      0      0      0      0   \n",
              " 4        0      0      0      0      0      0      0      0      0      0   \n",
              " ..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              " 637      0      0      0      0      0      0      0      0      0      0   \n",
              " 638      0      0      0      0      0      0      0      0      0      0   \n",
              " 639      0      0      0      0      0      0      0      0      0      0   \n",
              " 640      0      0      0      0      0      0      0      0      0      0   \n",
              " 641      0      0      0      0      0      0      0      0      0      0   \n",
              " \n",
              "      ...  49991  49992  49993  49994  49995  49996  49997  49998  49999  50000  \n",
              " 0    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 1    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 2    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 3    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 4    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " ..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
              " 637  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 638  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 639  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 640  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 641  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " \n",
              " [642 rows x 50001 columns],\n",
              " 'i_AMPA':      0      1      2      3      4      5      6      7      8      9      \\\n",
              " 0        0      0      0      0      0      0      0      0      0      0   \n",
              " 1        0      0      0      0      0      0      0      0      0      0   \n",
              " 2        0      0      0      0      0      0      0      0      0      0   \n",
              " 3        0      0      0      0      0      0      0      0      0      0   \n",
              " 4        0      0      0      0      0      0      0      0      0      0   \n",
              " ..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              " 637      0      0      0      0      0      0      0      0      0      0   \n",
              " 638      0      0      0      0      0      0      0      0      0      0   \n",
              " 639      0      0      0      0      0      0      0      0      0      0   \n",
              " 640      0      0      0      0      0      0      0      0      0      0   \n",
              " 641      0      0      0      0      0      0      0      0      0      0   \n",
              " \n",
              "      ...  49991  49992  49993  49994  49995  49996  49997  49998  49999  50000  \n",
              " 0    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 1    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 2    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 3    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 4    ...      0      0      0      0      0      0      0      0      0      0  \n",
              " ..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
              " 637  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 638  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 639  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 640  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " 641  ...      0      0      0      0      0      0      0      0      0      0  \n",
              " \n",
              " [642 rows x 50001 columns]}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_cell.write_seg_info_to_csv()"
      ],
      "metadata": {
        "id": "jR7zRkrkRPS5"
      },
      "id": "jR7zRkrkRPS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_segments_df=pd.read_csv(original_cell.output_folder_name+'/seg_info.csv')"
      ],
      "metadata": {
        "id": "CwAGcnISaxWj"
      },
      "id": "CwAGcnISaxWj",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(original_segments_df.columns)"
      ],
      "metadata": {
        "id": "fwkE0i_bbKdM",
        "outputId": "a9cf1f07-42d1-4471-b5a5-4dc4ec18e52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fwkE0i_bbKdM",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['seg', 'seg_id', 'bmtk_id', 'x', 'type', 'sec_id', 'nseg', 'Ra',\n",
            "       'seg_L', 'parent_seg_id', 'seg_elec_info', 'netcons_per_seg',\n",
            "       'netcon_density_per_seg'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_morphology(original_segments_df,\"/\"+original_cell.output_folder_name+\"morphology.svg\")"
      ],
      "metadata": {
        "id": "nMxpqZJ4a0f9",
        "outputId": "f7cd9425-f5f8-4823-f3b2-20a706b45539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        }
      },
      "id": "nMxpqZJ4a0f9",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Sec ID'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-439869800e1f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_morphology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_segments_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moriginal_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_folder_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"morphology.svg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-7246c629460e>\u001b[0m in \u001b[0;36mplot_morphology\u001b[0;34m(seg_df, savename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_morphology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'apic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sec ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         plt.plot(\n\u001b[1;32m      5\u001b[0m             \u001b[0mseg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'apic'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sec ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Coord X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Sec ID'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cell.get_recorder_data() #also creates directory"
      ],
      "metadata": {
        "id": "dieTnVipnJZb"
      },
      "id": "dieTnVipnJZb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell.__write_seg_info_to_csv() \n",
        "expanded_segments_df=pd.read_csv('/'+cell.output_folder_name+'/seg_info.csv')\n",
        "plot_morphology(expanded_segments_df,\"/\"+cell.output_folder_name+\"morphology.svg\")"
      ],
      "metadata": {
        "id": "nHQkNhj-RcPE"
      },
      "id": "nHQkNhj-RcPE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excitatory synapse density"
      ],
      "metadata": {
        "id": "9JH-R53uoav-"
      },
      "id": "9JH-R53uoav-"
    },
    {
      "cell_type": "code",
      "source": [
        "original_cell.plot_seg_heatmap(original_segments_df,color_column=\"exc_NetCon_density\")"
      ],
      "metadata": {
        "id": "S8KGpnxiDwTu"
      },
      "id": "S8KGpnxiDwTu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell.plot_seg_heatmap(expanded_segments_df,color_column=\"exc_NetCon_density\")"
      ],
      "metadata": {
        "id": "m_n715z1qiJf"
      },
      "id": "m_n715z1qiJf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inhibitory synapse density"
      ],
      "metadata": {
        "id": "hdt6s38Iocpx"
      },
      "id": "hdt6s38Iocpx"
    },
    {
      "cell_type": "code",
      "source": [
        "original_cell.plot_seg_heatmap(original_segments_df,color_column=\"inh_NetCon_density\")"
      ],
      "metadata": {
        "id": "fZAjr__-GLPP"
      },
      "id": "fZAjr__-GLPP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell.plot_seg_heatmap(expanded_segments_df,color_column=\"inh_NetCon_density\")"
      ],
      "metadata": {
        "id": "9nezb4KboYLu"
      },
      "id": "9nezb4KboYLu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import h5py\n",
        "# def createsegtracereport(reportname,dataname):\n",
        "#   try:\n",
        "#     os.remove(reportname) # reportname was string \" \"\n",
        "#   except:\n",
        "#     x = 1\n",
        "\n",
        "#   f = h5py.File(reportname,'w') #create a file in the w (write) mode #reportname was string ' '\n",
        "#   v = f.create_dataset(\"report/biophysical/data\", data = dataname)\n",
        "#   f.close()\n",
        "\n",
        "# # output files for analysis in another notebook\n",
        "# createsegtracereport(output_folder_name+'/v_report.h5', Vm.T)\n",
        "# createsegtracereport(output_folder_name+'/Ca_HVA.ica_report.h5',icah_data.T)\n",
        "# createsegtracereport(output_folder_name+'/Ca_LVAst.ica_report.h5',ical_data.T)\n",
        "# createsegtracereport(output_folder_name+'/Ih.ihcn_report.h5',ih_data.T)\n",
        "# createsegtracereport(output_folder_name+'/inmda_report.h5',i_NMDA_df.T)\n",
        "# createsegtracereport(output_folder_name+'/NaTa_t.gNaTa_t_report.h5',gNaTa_T_data.T)\n",
        "# createsegtracereport(output_folder_name+'/NaTa_t.ina_report.h5',ina_data.T)"
      ],
      "metadata": {
        "id": "0q8e1QQeewDO"
      },
      "id": "0q8e1QQeewDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#   os.remove(output_folder_name+\"/spikes.h5\")\n",
        "# except:\n",
        "#   x = 1\n",
        "\n",
        "# f = h5py.File(output_folder_name+'/spikes.h5','w') #create a file in the w (write) mode\n",
        "# v = f.create_dataset(\"spikes/biophysical/timestamps\", data = cell.spikes)\n",
        "\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "b7JdChnftUZa"
      },
      "id": "b7JdChnftUZa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adjust so that a folder is generated for complex and reduced cell and with simulation time as part of the folder name\n",
        "#maybe include syn distribution in name too"
      ],
      "metadata": {
        "id": "ALdGYUA_NPoR"
      },
      "id": "ALdGYUA_NPoR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-8EXYWZNkN23"
      },
      "id": "-8EXYWZNkN23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r <\"outputcontrol_4nbranch_10000NCs_114nsyn_88nseg\">.zip <\"/content/Stylized-Single-Cell-and-Extracellular-Potential/outputcontrol_4nbranch_10000NCs_114nsyn_88nseg\">"
      ],
      "metadata": {
        "id": "kKc8dUrDkV5r"
      },
      "id": "kKc8dUrDkV5r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_folder_zip=output_folder_name+'.zip'\n",
        "# # "
      ],
      "metadata": {
        "id": "d3bPzh0-GrVv"
      },
      "id": "d3bPzh0-GrVv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r $output_folder_zip $output_folder_name"
      ],
      "metadata": {
        "id": "ew3uW3oWkrXW"
      },
      "id": "ew3uW3oWkrXW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/Stylized-Single-Cell-and-Extracellular-Potential/outputcontrol_4nbranch_10000NCs_114nsyn_88nseg"
      ],
      "metadata": {
        "id": "cLL79lNOj6k_"
      },
      "id": "cLL79lNOj6k_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # expanded_segments_df.to_csv('expanded_segments_df.csv')\n",
        "# # from google.colab import files\n",
        "# # Save the DataFrame to the specified file path\n",
        "# expanded_segments_df.to_csv('/content/drive/MyDrive/expanded_segments_df.csv', index=False)"
      ],
      "metadata": {
        "id": "4j0-Wsw1n_CQ"
      },
      "id": "4j0-Wsw1n_CQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %ls"
      ],
      "metadata": {
        "id": "yK9Rj0Xlur_d"
      },
      "id": "yK9Rj0Xlur_d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp $output_folder_zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "Dm0ea7FB_Hw4"
      },
      "id": "Dm0ea7FB_Hw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd outputcontrol_4nbranch_10000NCs_114nsyn_88nseg/"
      ],
      "metadata": {
        "id": "ykT3lDGMuv3l"
      },
      "id": "ykT3lDGMuv3l",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}