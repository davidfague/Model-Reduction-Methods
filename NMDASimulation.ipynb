{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidfague/Model_Reduction_Methods/blob/main/NMDASimulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9de89f",
      "metadata": {
        "id": "7a9de89f"
      },
      "source": [
        "# Simulation for generating data for dendritic spike analysis\n",
        "##Recorded Currents:\n",
        "\n",
        "Na,K,Ca,ih,...\n",
        "\n",
        "## synapse distribution:\n",
        "\n",
        "10,000 random. will update to include more realistic algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSX7LmGzszzf",
      "metadata": {
        "id": "MSX7LmGzszzf"
      },
      "source": [
        "#### Download modules from Github"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neuron"
      ],
      "metadata": {
        "id": "-urE9jrxNdxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342ecefe-e98f-4e24-ab2a-79c88feabff3"
      },
      "id": "-urE9jrxNdxm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neuron\n",
            "  Downloading NEURON-8.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.9/dist-packages (from neuron) (1.22.4)\n",
            "Installing collected packages: neuron\n",
            "Successfully installed neuron-8.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neuron_reduce"
      ],
      "metadata": {
        "id": "BHjnRgPsN0g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1787fd-75a5-41da-8c04-ce9ed7d19b02"
      },
      "id": "BHjnRgPsN0g2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neuron_reduce\n",
            "  Downloading neuron_reduce-0.0.7-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: neuron_reduce\n",
            "Successfully installed neuron_reduce-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/davidfague/Model_Reduction_Methods.git"
      ],
      "metadata": {
        "id": "t4dVzA0WLZ9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e59242d-297c-471f-9a03-e736bcdb4242"
      },
      "id": "t4dVzA0WLZ9F",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Model_Reduction_Methods'...\n",
            "remote: Enumerating objects: 749, done.\u001b[K\n",
            "remote: Counting objects: 100% (435/435), done.\u001b[K\n",
            "remote: Compressing objects: 100% (260/260), done.\u001b[K\n",
            "remote: Total 749 (delta 260), reused 303 (delta 174), pack-reused 314\u001b[K\n",
            "Receiving objects: 100% (749/749), 5.40 MiB | 14.13 MiB/s, done.\n",
            "Resolving deltas: 100% (411/411), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Model_Reduction_Methods/\n",
        "\n",
        "#import reduction and expansion functions\n",
        "from test_neuron_reduce.subtree_reductor_func import subtree_reductor\n",
        "from cable_expander_func import cable_expander\n",
        "\n",
        "#import recording functions\n",
        "from stylized_module.recorder import Recorder\n",
        "\n",
        "#import analysis functions\n",
        "from utils import make_seg_df,generate_stylized_geometry,make_reduced_seg_df,plot_morphology,check_connectivity,generate_reduced_cell_seg_coords, create_seg_var_report\n",
        "\n",
        "\n",
        "# from modeling_module.synapses import Synapse, Listed_Synapse\n",
        "from modeling_module.cell_model import cell_model\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cY9U_KPwLVAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c38d9b-aa57-4aa4-8935-3428d5ff538e"
      },
      "id": "cY9U_KPwLVAz",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Model_Reduction_Methods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd expand_example"
      ],
      "metadata": {
        "id": "vXTRrX-ILq6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e134f64-0d14-4f34-9ef2-f0444d757d26"
      },
      "id": "vXTRrX-ILq6D",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Model_Reduction_Methods/expand_example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the mod files\n",
        "!nrnivmodl mod"
      ],
      "metadata": {
        "id": "Dm-MdGeb6uZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f52db7b-da7f-42dd-e62d-528044c23d73"
      },
      "id": "Dm-MdGeb6uZB",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Model_Reduction_Methods/expand_example\n",
            "Mod files: \"mod/mod/CaDynamics_E2.mod\" \"mod/mod/Ca_HVA.mod\" \"mod/mod/Ca_LVAst.mod\" \"mod/mod/epsp.mod\" \"mod/mod/Ih.mod\" \"mod/mod/Im.mod\" \"mod/mod/K_Pst.mod\" \"mod/mod/K_Tst.mod\" \"mod/mod/Nap_Et2.mod\" \"mod/mod/NaTa_t.mod\" \"mod/mod/NaTs2_t.mod\" \"mod/mod/SK_E2.mod\" \"mod/mod/SKv3_1.mod\"\n",
            "\n",
            "Creating 'x86_64' directory for .o files.\n",
            "\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Ca_LVAst.mod\n",
            " -> \u001b[32mCompiling\u001b[0m mod_func.cpp\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Ca_HVA.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/CaDynamics_E2.mod\n",
            "Translating CaDynamics_E2.mod into /content/Model_Reduction_Methods/expand_example/x86_64/CaDynamics_E2.c\n",
            "Translating Ca_LVAst.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Ca_LVAst.c\n",
            "Thread Safe\n",
            "Translating Ca_HVA.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Ca_HVA.c\n",
            "Thread Safe\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/epsp.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Im.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Ih.mod\n",
            "Translating epsp.mod into /content/Model_Reduction_Methods/expand_example/x86_64/epsp.c\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/K_Pst.mod\n",
            "Translating Im.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Im.c\n",
            "Translating Ih.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Ih.c\n",
            "Thread Safe\n",
            "Thread Safe\n",
            "Translating K_Pst.mod into /content/Model_Reduction_Methods/expand_example/x86_64/K_Pst.c\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/K_Tst.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/Nap_Et2.mod\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/NaTa_t.mod\n",
            "Translating K_Tst.mod into /content/Model_Reduction_Methods/expand_example/x86_64/K_Tst.c\n",
            "Translating NaTa_t.mod into /content/Model_Reduction_Methods/expand_example/x86_64/NaTa_t.c\n",
            "Thread Safe\n",
            "Thread Safe\n",
            "Translating Nap_Et2.mod into /content/Model_Reduction_Methods/expand_example/x86_64/Nap_Et2.c\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/NaTs2_t.mod\n",
            "Thread Safe\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/SK_E2.mod\n",
            "Translating NaTs2_t.mod into /content/Model_Reduction_Methods/expand_example/x86_64/NaTs2_t.c\n",
            " -> \u001b[32mNMODL\u001b[0m ../mod/SKv3_1.mod\n",
            "Thread Safe\n",
            "Translating SK_E2.mod into /content/Model_Reduction_Methods/expand_example/x86_64/SK_E2.c\n",
            "Thread Safe\n",
            "Translating SKv3_1.mod into /content/Model_Reduction_Methods/expand_example/x86_64/SKv3_1.c\n",
            "Thread Safe\n",
            " -> \u001b[32mCompiling\u001b[0m CaDynamics_E2.c\n",
            " -> \u001b[32mCompiling\u001b[0m Ca_HVA.c\n",
            " -> \u001b[32mCompiling\u001b[0m Ca_LVAst.c\n",
            " -> \u001b[32mCompiling\u001b[0m epsp.c\n",
            " -> \u001b[32mCompiling\u001b[0m Ih.c\n",
            " -> \u001b[32mCompiling\u001b[0m Im.c\n",
            " -> \u001b[32mCompiling\u001b[0m K_Pst.c\n",
            " -> \u001b[32mCompiling\u001b[0m K_Tst.c\n",
            " -> \u001b[32mCompiling\u001b[0m Nap_Et2.c\n",
            " -> \u001b[32mCompiling\u001b[0m NaTa_t.c\n",
            " -> \u001b[32mCompiling\u001b[0m NaTs2_t.c\n",
            " -> \u001b[32mCompiling\u001b[0m SK_E2.c\n",
            " -> \u001b[32mCompiling\u001b[0m SKv3_1.c\n",
            " => \u001b[32mLINKING\u001b[0m shared library ./libnrnmech.so\n",
            " => \u001b[32mLINKING\u001b[0m executable ./special LDFLAGS are:    -pthread\n",
            "Successfully created x86_64/special\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a84fbea",
      "metadata": {
        "id": "2a84fbea"
      },
      "source": [
        "## Setup smiulation parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4be7f6fd",
      "metadata": {
        "id": "4be7f6fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9f1fc1-4b74-4550-fd71-5b075f03997e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from neuron import h\n",
        "from scipy import signal\n",
        "from IPython.display import display, clear_output\n",
        "from ipywidgets import interactive_output, HBox, VBox, Label, Layout\n",
        "\n",
        "from __future__ import division\n",
        "from neuron import gui,h\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "h.load_file('stdrun.hoc')\n",
        "# h.nrn_load_dll(paths.COMPILED_LIBRARY_REDUCED_ORDER)  # choose the set of mechanisms\n",
        "h.nrn_load_dll('./x86_64/.libs/libnrnmech.so')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7927a07b",
      "metadata": {
        "id": "7927a07b"
      },
      "source": [
        "### Create a cell with reduced morphology"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "s0gis1f1OdWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1cc9741-04bf-46ac-dcbe-61bf7a9eb7cc"
      },
      "id": "s0gis1f1OdWX",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cell1.asc  example_expand.py  L5PCtemplate.hoc  \u001b[0m\u001b[01;34mx86_64\u001b[0m/\n",
            "Cell.hoc   L5PCbiophys3.hoc   \u001b[01;34mmod\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "18a97eeb",
      "metadata": {
        "id": "18a97eeb"
      },
      "outputs": [],
      "source": [
        "\n",
        "h.load_file('L5PCbiophys3.hoc') # load membrane biophysics\n",
        "h.load_file(\"import3d.hoc\") #load 3d morphology\n",
        "\n",
        "\n",
        "# Create a cell object\n",
        "h.load_file('L5PCtemplate.hoc') # load template for generating object\n",
        "complex_cell = h.L5PCtemplate('cell1.asc') # generate object\n",
        "\n",
        "#specify some parameters\n",
        "h.celsius = 37\n",
        "h.v_init = complex_cell.soma[0].e_pas\n",
        "\n",
        "#Add synapses to the complex model\n",
        "synapses_list, netstims_list, netcons_list, randoms_list = [], [], [] ,[]\n",
        "\n",
        "all_segments = [i for j in map(list,list(complex_cell.apical)) for i in j] + [i for j in map(list,list(complex_cell.basal)) for i in j]\n",
        "len_per_segment = np.array([seg.sec.L/seg.sec.nseg for seg in all_segments])\n",
        "rnd = np.random.RandomState(10)\n",
        "for i in range(10000):\n",
        "    seg_for_synapse = rnd.choice(all_segments,   p=len_per_segment/sum(len_per_segment)) #choose a random segment with probability based on the length of segment\n",
        "    synapses_list.append(h.Exp2Syn(seg_for_synapse))\n",
        "    if rnd.uniform()<0.85: # 85% synapses are excitatory\n",
        "        e_syn, tau1, tau2, spike_interval, syn_weight = 0, 0.3, 1.8,  1000/2.5, 0.0016\n",
        "    else: #inhibitory case\n",
        "        e_syn, tau1, tau2, spike_interval, syn_weight = -86, 1,   8,   1000/15.0, 0.0008\n",
        "    #set synaptic varibales\n",
        "    synapses_list[i].e, synapses_list[i].tau1, synapses_list[i].tau2 = e_syn, tau1, tau2\n",
        "    #set netstim variables\n",
        "    netstims_list.append(h.NetStim())\n",
        "    netstims_list[i].interval, netstims_list[i].number, netstims_list[i].start, netstims_list[i].noise = spike_interval, 9e9, 100, 1\n",
        "    #set random\n",
        "    randoms_list.append(h.Random())\n",
        "    randoms_list[i].Random123(i)\n",
        "    randoms_list[i].negexp(1)\n",
        "    netstims_list[i].noiseFromRandom(randoms_list[i])       \n",
        "    #set netcon varibales \n",
        "    netcons_list.append(h.NetCon(netstims_list[i], synapses_list[i] ))\n",
        "    netcons_list[i].delay, netcons_list[i].weight[0] = 0, syn_weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reduce each dendritic subtree to a single cable\n",
        "reduced_cell, synapses_list, netcons_list, txt = subtree_reductor(complex_cell, synapses_list, netcons_list, reduction_frequency=0,return_seg_to_seg=True)"
      ],
      "metadata": {
        "id": "o76VOo9ACebo"
      },
      "id": "o76VOo9ACebo",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check synapses_list with netcons_list\n",
        "for netcon in netcons_list:\n",
        "  syn=netcon.syn()\n",
        "  if syn not in synapses_list:\n",
        "    print(syn, netcon)"
      ],
      "metadata": {
        "id": "eAWQMC4tGRFj"
      },
      "id": "eAWQMC4tGRFj",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices=[109,122,130,309,360,364,446,629,290,791,851,930,951,967,975,981,1047,1104]\n",
        "for index in indices:\n",
        "  print(netcons_list[index], netcons_list[index].syn(),netcons_list[index].syn().get_segment())\n",
        "\n"
      ],
      "metadata": {
        "id": "Kpxld90XHb1d",
        "outputId": "dd52a253-cb0f-4aba-d1a5-004368f25d66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Kpxld90XHb1d",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetCon[109] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[122] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[130] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[309] Exp2Syn[309] model[0].apic[0](0.289474)\n",
            "NetCon[360] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[364] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[446] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[629] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[290] Exp2Syn[47] model[0].apic[0](0.657895)\n",
            "NetCon[791] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[851] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[930] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[951] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[967] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[975] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[981] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[1047] Exp2Syn[109] model[0].apic[0](0.289474)\n",
            "NetCon[1104] Exp2Syn[309] model[0].apic[0](0.289474)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#expand cables to idealized dendritic trees\n",
        "sections_to_expand = [reduced_cell.hoc_model.apic[0]] # expand apical cylinder\n",
        "furcations_x=[0.289004] #chose using the location of the mapped nexus branching segment\n",
        "nbranches=[4]\n",
        "reduced_dendritic_cell, synapses_list, netcons_list, txt = cable_expander(reduced_cell, sections_to_expand, furcations_x, nbranches, \n",
        "                                                                          synapses_list, netcons_list, reduction_frequency=0,return_seg_to_seg=True)"
      ],
      "metadata": {
        "id": "BQpLyTABCjWT"
      },
      "id": "BQpLyTABCjWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check synapses_list with netcons_list\n",
        "for netcon in netcons_list:\n",
        "  syn=netcon.syn()\n",
        "  if syn not in synapses_list:\n",
        "    print(syn, netcon)"
      ],
      "metadata": {
        "id": "HlRaujdIGUqF"
      },
      "id": "HlRaujdIGUqF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # try branching the branches - does not work yet\n",
        "# sections_to_expand = [reduced_dendritic_cell.apic[1],reduced_dendritic_cell.apic[2],reduced_dendritic_cell.apic[3],reduced_dendritic_cell.apic[4]]\n",
        "# furcations_x=[0.50,0.50,0.50,0.50]\n",
        "# nbranches=[4,4,4,4]\n",
        "# reduced_dendritic_cell, synapses_list, netcons_list, txt = cable_expander(reduced_dendritic_cell, sections_to_expand, furcations_x, nbranches, \n",
        "#                                                                           synapses_list, netcons_list, reduction_frequency=0,return_seg_to_seg=True)"
      ],
      "metadata": {
        "id": "1Ap8Gp-mCm-4"
      },
      "id": "1Ap8Gp-mCm-4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check seg mapping\n",
        "# for i in txt:\n",
        "#   print(i,\"was mapped to\",txt[i])"
      ],
      "metadata": {
        "id": "LPXtOiycci5T"
      },
      "id": "LPXtOiycci5T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use defined cell_model python class for generating 3d coordinates, recording ECP, 'book-keeping' etc...\n",
        "import random\n",
        "random.seed(2)\n",
        "cell = cell_model(reduced_dendritic_cell,synapses_list=synapses_list,netcons_list=netcons_list,spike_threshold = 10)\n",
        "# cell._nbranch=4"
      ],
      "metadata": {
        "id": "NndM90Z9SrU7"
      },
      "id": "NndM90Z9SrU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for sec in cell.all:\n",
        "#   for seg in sec:\n",
        "#     for pp in seg.point_processes():\n",
        "#       print(pp)"
      ],
      "metadata": {
        "id": "PkAbftjKU8wr"
      },
      "id": "PkAbftjKU8wr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #potentially deleted synapses?\n",
        "# for netcon in netcons_list:\n",
        "#   if netcon.syn() not in synapses_list:\n",
        "#     print(netcon)\n",
        "#     print(netcon.syn() in synapses_list)\n",
        "#     print(netcon.syn())"
      ],
      "metadata": {
        "id": "MQOphn2sWsw4"
      },
      "id": "MQOphn2sWsw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for syn in synapses_list:\n",
        "#   print(syn,syn.get_segment().point_processes())"
      ],
      "metadata": {
        "id": "kOgZ9mciVC5Z"
      },
      "id": "kOgZ9mciVC5Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #create second original model\n",
        "\n",
        "# original_cell = h.L5PCtemplate('cell1.asc')\n",
        "# synapses_list, netstims_list, netcons_list, randoms_list = [], [], [] ,[]\n",
        "\n",
        "# all_segments = [i for j in map(list,list(original_cell.apical)) for i in j] + [i for j in map(list,list(original_cell.basal)) for i in j]\n",
        "# len_per_segment = np.array([seg.sec.L/seg.sec.nseg for seg in all_segments])\n",
        "# rnd = np.random.RandomState(10)\n",
        "# for i in range(10000):\n",
        "#     seg_for_synapse = rnd.choice(all_segments,   p=len_per_segment/sum(len_per_segment)) #choose a random segment with probability based on the length of segment\n",
        "#     synapses_list.append(h.Exp2Syn(seg_for_synapse))\n",
        "#     if rnd.uniform()<0.85: # 85% synapses are excitatory\n",
        "#         e_syn, tau1, tau2, spike_interval, syn_weight = 0, 0.3, 1.8,  1000/2.5, 0.0016\n",
        "#     else: #inhibitory case\n",
        "#         e_syn, tau1, tau2, spike_interval, syn_weight = -86, 1,   8,   1000/15.0, 0.0008\n",
        "#     #set synaptic varibales\n",
        "#     synapses_list[i].e, synapses_list[i].tau1, synapses_list[i].tau2 = e_syn, tau1, tau2\n",
        "#     #set netstim variables\n",
        "#     netstims_list.append(h.NetStim())\n",
        "#     netstims_list[i].interval, netstims_list[i].number, netstims_list[i].start, netstims_list[i].noise = spike_interval, 9e9, 100, 1\n",
        "#     #set random\n",
        "#     randoms_list.append(h.Random())\n",
        "#     randoms_list[i].Random123(i)\n",
        "#     randoms_list[i].negexp(1)\n",
        "#     netstims_list[i].noiseFromRandom(randoms_list[i])       \n",
        "#     #set netcon varibales \n",
        "#     netcons_list.append(h.NetCon(netstims_list[i], synapses_list[i] ))\n",
        "#     netcons_list[i].delay, netcons_list[i].weight[0] = 0, syn_weight\n",
        "# original_model = cell_model(original_cell,gen_3d=False,spike_threshold = 10)\n",
        "# print(synapses_list)"
      ],
      "metadata": {
        "id": "kHCBMJHzUyAY"
      },
      "id": "kHCBMJHzUyAY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_reduced_seg_df(cell,\"segments_expanded.csv\") #need to improve make_reduced_seg_df\n",
        "expanded_segments_df=pd.read_csv(\"segments_expanded.csv\")\n",
        "plot_morphology(expanded_segments_df,\"expanded_morphology.svg\")\n",
        "\n",
        "# change to complex cell\n",
        "# make_reduced_seg_df(cell,\"segments_expanded.csv\") #need to improve make_reduced_seg_df\n",
        "# expanded_segments_df=pd.read_csv(\"segments_expanded.csv\")\n",
        "# plot_morphology(expanded_segments_df,\"expanded_morphology.svg\")"
      ],
      "metadata": {
        "id": "JjM51nSARop9"
      },
      "id": "JjM51nSARop9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(cell)"
      ],
      "metadata": {
        "id": "akYHQOGDKiW5"
      },
      "id": "akYHQOGDKiW5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "if RunningInCOLAB:\n",
        "    !pip install neuron==8.0.0 &> /dev/null\n",
        "    os.chdir('/content')\n",
        "    if not os.path.isdir('Stylized-Single-Cell-and-Extracellular-Potential'):\n",
        "        !git clone https://github.com/chenziao/Stylized-Single-Cell-and-Extracellular-Potential.git &> /dev/null \n",
        "    os.chdir('Stylized-Single-Cell-and-Extracellular-Potential')\n",
        "    %ls"
      ],
      "metadata": {
        "id": "D6vyubrwOvA_"
      },
      "id": "D6vyubrwOvA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union, List, Tuple\n",
        "\n",
        "\n",
        "class Recorder(object):\n",
        "    \"\"\"A module for recording variables\"\"\"\n",
        "\n",
        "    def __init__(self, obj_list: Union[object, List[object], Tuple[object], np.ndarray], var_name: str = 'v') -> None:\n",
        "        \"\"\"\n",
        "        obj_list: list of (or a single) target objects\n",
        "        var_name: string of variable to be recorded\n",
        "        \"\"\"\n",
        "        self.single = not isinstance(obj_list, (list, tuple, np.ndarray))\n",
        "        self.obj_list = obj_list\n",
        "        self.var_name = var_name\n",
        "        self.vectors = None\n",
        "        self.setup_recorder()\n",
        "\n",
        "    def setup_recorder(self) -> None:\n",
        "        size = [round(h.tstop / h.dt) + 1] if hasattr(h, 'tstop') else []\n",
        "        attr_name = '_ref_' + self.var_name\n",
        "        if self.single:\n",
        "            self.vectors = h.Vector(*size).record(getattr(self.obj_list, attr_name))\n",
        "        else:\n",
        "            self.vectors = [h.Vector(*size).record(getattr(obj, attr_name)) for obj in self.obj_list]\n",
        "\n",
        "    def as_numpy(self, copy: bool = True) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Return a numpy 2d-array of recording, n objects-by-time\n",
        "        Return a 1d-array if a single object is being recorded\n",
        "        \"\"\"\n",
        "        if self.single:\n",
        "            x = self.vectors.as_numpy()\n",
        "        else:\n",
        "            x = np.array([v.as_numpy() for v in self.vectors])\n",
        "        if copy:\n",
        "            x = x.copy()\n",
        "        return x"
      ],
      "metadata": {
        "id": "H-rE_7J2QNsd"
      },
      "id": "H-rE_7J2QNsd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#insert unused channels for recorder\n",
        "for sec in cell.all:\n",
        "  if not hasattr(sec(0.5),'gNaTa_t_NaTa_t'):\n",
        "    sec.insert('NaTa_t')\n",
        "    for seg in sec:\n",
        "      seg.NaTa_t.gNaTa_tbar=0\n",
        "    print(sec)"
      ],
      "metadata": {
        "id": "ln05q5x0GGIO"
      },
      "id": "ln05q5x0GGIO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#insert unused channels for recorder\n",
        "for sec in cell.all:\n",
        "  if not hasattr(sec(0.5),'ica_Ca_LVAst'):\n",
        "    sec.insert('Ca_LVAst')\n",
        "    for seg in sec:\n",
        "      seg.Ca_LVAst.gCa_LVAstbar=0\n",
        "    print(sec)"
      ],
      "metadata": {
        "id": "w1rmShPhIMP1"
      },
      "id": "w1rmShPhIMP1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#insert unused channels for recorder\n",
        "for sec in cell.all:\n",
        "  if not hasattr(sec(0.5),'ica_Ca_HVA'):\n",
        "    sec.insert('Ca_HVA')\n",
        "    for seg in sec:\n",
        "      seg.Ca_HVA.gCa_HVAbar"
      ],
      "metadata": {
        "id": "BkKbsI9aIgAH"
      },
      "id": "BkKbsI9aIgAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#insert unused channels for recorder\n",
        "for sec in cell.all:\n",
        "  if not hasattr(sec(0.5),'ihcn_Ih'):\n",
        "    sec.insert('Ih')\n",
        "    for seg in sec:\n",
        "      seg.Ih.gIhbar"
      ],
      "metadata": {
        "id": "Qq7bpFp3Iq-u"
      },
      "id": "Qq7bpFp3Iq-u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define simulation parameters\n",
        "# tstop = 6000  # ms simulation duration\n",
        "tstop = 5000\n",
        "dt = 0.1  # ms\n",
        "h.tstop = tstop\n",
        "h.dt = dt\n",
        "h.steps_per_ms = 1/h.dt\n",
        "\n",
        "#define parameters that may be used in notebook\n",
        "nseg = len(cell.segments)\n",
        "numTstep = int(tstop/dt)\n",
        "\n",
        "#set up recorders\n",
        "# gNaTa_T = Recorder(obj_list = original_model.segments, var_name = 'gNaTa_t_NaTa_t')\n",
        "# ical = Recorder(obj_list = original_model.segments, var_name = 'ica_Ca_LVAst')\n",
        "# icah = Recorder(obj_list = original_model.segments, var_name = 'ica_Ca_HVA')\n",
        "# ih = Recorder(obj_list = original_model.segments, var_name = 'ihcn_Ih')\n",
        "# Vm = Recorder(obj_list = original_model.segments)\n",
        "\n",
        "# set up recorders\n",
        "gNaTa_T = Recorder(obj_list = cell.segments, var_name = 'gNaTa_t_NaTa_t')\n",
        "ical = Recorder(obj_list = cell.segments, var_name = 'ica_Ca_LVAst')\n",
        "icah = Recorder(obj_list = cell.segments, var_name = 'ica_Ca_HVA')\n",
        "ih = Recorder(obj_list = cell.segments, var_name = 'ihcn_Ih')\n",
        "Vm = Recorder(obj_list = cell.segments)"
      ],
      "metadata": {
        "id": "1tRGr5NEOs59"
      },
      "id": "1tRGr5NEOs59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run simulation\n",
        "\n",
        "timestart=time.time()\n",
        "h.run()\n",
        "timestop=time.time()\n",
        "t = h.t # was t=h.t() but 'float' object not callable\n",
        "elapsedtime=timestop-timestart\n",
        "simtime=tstop/1000 #convert from ms to s\n",
        "# totaltime= elapsedtime+elapseddeftime\n",
        "print('It took',round(elapsedtime),'sec to run a',simtime,'sec simulation.')\n",
        "# print('The total runtime was',round(totaltime),'sec')"
      ],
      "metadata": {
        "id": "U5azilotOvGY"
      },
      "id": "U5azilotOvGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get data from recorders\n",
        "ih_data = ih.as_numpy()\n",
        "gNaTa_T_data = gNaTa_T.as_numpy()\n",
        "icah_data = icah.as_numpy()\n",
        "ical_data = ical.as_numpy()\n",
        "Vm = Vm.as_numpy()"
      ],
      "metadata": {
        "id": "z1S1XJgLegXz"
      },
      "id": "z1S1XJgLegXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "def createsegtracereport(reportname,dataname):\n",
        "  try:\n",
        "    os.remove(reportname) # reportname was string \" \"\n",
        "  except:\n",
        "    x = 1\n",
        "\n",
        "  f = h5py.File(reportname,'w') #create a file in the w (write) mode #reportname was string ' '\n",
        "  v = f.create_dataset(\"report/biophysical/data\", data = dataname)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "hhPWsoCPeg4l"
      },
      "id": "hhPWsoCPeg4l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate inmda\n",
        "#since synapse list has combined synapses for computational efficiency, we must use NetCon list/unique spike trains to determine how many synapses were mapped\n",
        "NetCon_per_seg=[0]*nseg\n",
        "inh_NetCon_per_seg=[0]*nseg\n",
        "exc_NetCon_per_seg=[0]*nseg\n",
        "\n",
        "i_NMDA_bySeg= [[0] * (numTstep+1) ] * nseg # need to implement inmda recording\n",
        "\n",
        "v_rest=-60 #choose v_rest for categorizing inh/exc synapses\n",
        "\n",
        "#calculate number of synapses for each segment (may want to divide by segment length afterward to get synpatic density)\n",
        "for netcon in netcons_list:\n",
        "  syn=netcon.syn()\n",
        "  if syn in synapses_list:\n",
        "    syn_seg_id=cell.segments.index(netcon.syn().get_segment())\n",
        "    if syn in cell.segments[syn_seg_id].point_processes():\n",
        "      NetCon_per_seg[syn_seg_id]+=1 # get synapses per segment\n",
        "      # NetCon_per_seg[syn_seg_id].append(netcon) # possible implementation if needing objects per segment\n",
        "      if syn.e > v_rest:\n",
        "        exc_NetCon_per_seg[syn_seg_id]+=1\n",
        "        # exc_NetCon_per_seg[syn_seg_id].append(netcon)# possible implementation if needing objects per segment\n",
        "      else:\n",
        "        inh_NetCon_per_seg[syn_seg_id]+=1\n",
        "        # inh_NetCon_per_seg[syn_seg_id].append(netcon)# possible implementation if needing objects per segment\n",
        "    else:\n",
        "      print(\"Warning: synapse not in designated segment's point processes\")\n",
        "\n",
        "  else:\n",
        "    print(\"Warning: potentially deleted synapse:\",\"|NetCon obj:\",netcon,\"|Synapse obj:\",syn,\"the NetCon's synapse is not in synapses_list. Check corresponding original cell's NetCon for location, etc.\")\n",
        "\n",
        "#extract inmda from each segment # can be adjusted for gaba synapses or alpha synapses # only paired nmda/ampa synapses have vec_list[1]\n",
        "for synapse in cell.synapse:\n",
        "  try:\n",
        "    i_NMDA = np.array(synapse.rec_vec.vec_list[1])            #current = numpy array of NEURON Vector of current NMDA current at synapse j  \n",
        "    seg = synapse.get_segment_id()                            #seg = the segment in which synapse j is located \n",
        "    #print('first try')\n",
        "    \n",
        "    try:\n",
        "      i_NMDA_bySeg[seg] = i_NMDA_bySeg[seg] + i_NMDA    \n",
        "      #print('second try')                                           #Sum current over each segment\n",
        "    except: \n",
        "      pass                                                                   #Except needed as some synpases do not have NMDA currrent and throw an error when called\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "cbYSa3y5cKCY"
      },
      "id": "cbYSa3y5cKCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# numSyn = len(cell.synapse)\n",
        "\n",
        "# excSynPerSeg = [0]*nseg\n",
        "# inhSynPerSeg = [0]*nseg\n",
        "# excSynPerSegL = [0]*nseg\n",
        "# inhSynPerSegL = [0]*nseg\n",
        "# SynParentSeg = []\n",
        "# SourcePop = []\n",
        "# SynType = []\n",
        "# SynDist = []\n",
        "\n",
        "# # i_NMDA_bySeg= [[0] * (numTstep+1) ] * nseg\n",
        "\n",
        "#print(len(sim.cells[0].injection))\n",
        "# for j in range(numSyn):\n",
        "#   seg = cell.synapse[j].get_segment_id()\n",
        "#   SynParentSeg.append(seg)\n",
        "#   # SynType.append(AllSegType[seg])\n",
        "#   # SynDist.append(AllSegDist[seg])\n",
        "\n",
        "#   if(cell.synapse[j].syntype == 'exc'):\n",
        "#     excSynPerSeg[seg] += 1\n",
        "#     SourcePop.append('exc_stim')\n",
        "#   else:\n",
        "#     inhSynPerSeg[seg] += 1\n",
        "#     SourcePop.append('dist_inh_stim')\n",
        "\n",
        "#   try:\n",
        "#     i_NMDA = np.array(cell.injection[j].rec_vec.vec_list[1])            #current = numpy array of NEURON Vector of current NMDA current at synapse j  \n",
        "#     seg = cell.synapse[j].get_segment_id()                            #seg = the segment in which synapse j is located \n",
        "#     #print('first try')\n",
        "    \n",
        "#     try:\n",
        "#       i_NMDA_bySeg[seg] = i_NMDA_bySeg[seg] + i_NMDA    \n",
        "#       #print('second try')                                           #Sum current over each segment\n",
        "#     except: \n",
        "#       x = j                                                                     #Except needed as some synpases do not have NMDA currrent and throw an error when called\n",
        "#   except:\n",
        "#     x = j"
      ],
      "metadata": {
        "id": "ttUgHkqMe66r"
      },
      "id": "ttUgHkqMe66r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i_NMDA_bySeg[0]"
      ],
      "metadata": {
        "id": "PVRyx4gIBOd5"
      },
      "id": "PVRyx4gIBOd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Should add \"syn per seg\" to seg data frame #also can try doing SynPerSeg/segL to have more of a density metric\n",
        "def plotSynDensity(SynPerSeg):\n",
        "  plt.figure(figsize=(4,10))\n",
        "  ax = plt.scatter(expanded_segments_df[\"Coord X\"], expanded_segments_df[\"Coord Y\"],c = SynPerSeg[0:nseg],cmap='jet',)\n",
        "  plt.vlines(110,400,500)\n",
        "  plt.text(0,450,'100 um')\n",
        "  plt.hlines(400,110,210)\n",
        "  plt.text(110,350,'100 um')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  cbar = plt.colorbar()\n",
        "  cbar.ax.set_ylabel('Synapses per segment', rotation=270)\n",
        "\n",
        "  plt.box(False)\n",
        "  plt.savefig('exc_syns.svg')\n",
        "\n",
        "def plotSynDensityNoSoma(SynPerSeg):\n",
        "  plt.figure(figsize=(4,10))\n",
        "  ax = plt.scatter(expanded_segments_df[\"Coord X\"][1:], expanded_segments_df[\"Coord Y\"][1:],c = SynPerSeg[1:nseg],cmap='jet',)\n",
        "  plt.vlines(110,400,500)\n",
        "  plt.text(0,450,'100 um')\n",
        "  plt.hlines(400,110,210)\n",
        "  plt.text(110,350,'100 um')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  cbar = plt.colorbar()\n",
        "  cbar.ax.set_ylabel('Synapses per segment', rotation=270)\n",
        "\n",
        "  plt.box(False)\n",
        "  plt.savefig('inh_syns.svg')"
      ],
      "metadata": {
        "id": "faSg7ZDtOPbz"
      },
      "id": "faSg7ZDtOPbz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotSynDensity(exc_NetCon_per_seg) #len(cell.segments) #low number of segments makes plot look odd."
      ],
      "metadata": {
        "id": "gx5X8SydOTfY"
      },
      "id": "gx5X8SydOTfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotSynDensityNoSoma(inh_NetCon_per_seg) # if soma has too many inh synapses the heatmap is not as interpretable"
      ],
      "metadata": {
        "id": "qZ8I7m05OTw3"
      },
      "id": "qZ8I7m05OTw3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i_NMDA_df = pd.DataFrame(i_NMDA_bySeg) * 1000"
      ],
      "metadata": {
        "id": "RiMm8UZxejid"
      },
      "id": "RiMm8UZxejid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder_name=str(nbranches[0])+\"nbranch_outputcontrol_\"+str(int(len(cell.synapse)))+\"nsyn\"#+modelname  #include model name in output foler name (ex. original cell, reduced cell, expanded cell)\n",
        "#create output folder\n",
        "import os\n",
        "if not os.path.exists(output_folder_name):\n",
        "   os.makedirs(output_folder_name)\n",
        "\n",
        "print(output_folder_name)\n",
        "os.chdir(output_folder_name)"
      ],
      "metadata": {
        "id": "OEc0bW_GAXFL"
      },
      "id": "OEc0bW_GAXFL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create output folder\n",
        "import os\n",
        "if not os.path.exists(output_folder_name):\n",
        "   os.makedirs(output_folder_name)"
      ],
      "metadata": {
        "id": "689dzbn3Aeg1"
      },
      "id": "689dzbn3Aeg1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output files for analysis in another notebook\n",
        "createsegtracereport('v_report.h5', Vm.T)\n",
        "createsegtracereport('Ca_HVA.ica_report.h5',icah_data.T)\n",
        "createsegtracereport('Ca_LVAst.ica_report.h5',ical_data.T)\n",
        "createsegtracereport('Ih.ihcn_report.h5',ih_data.T)\n",
        "createsegtracereport('inmda_report.h5',i_NMDA_df.T)\n",
        "createsegtracereport('NaTa_t.gNaTa_t_report.h5',gNaTa_T_data.T)"
      ],
      "metadata": {
        "id": "0q8e1QQeewDO"
      },
      "id": "0q8e1QQeewDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "GbgVoXnMAUEi"
      },
      "id": "GbgVoXnMAUEi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adjust so that a folder is generated for complex and reduced cell and with simulation time as part of the folder name\n",
        "#maybe include syn distribution in name too"
      ],
      "metadata": {
        "id": "ALdGYUA_NPoR"
      },
      "id": "ALdGYUA_NPoR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}